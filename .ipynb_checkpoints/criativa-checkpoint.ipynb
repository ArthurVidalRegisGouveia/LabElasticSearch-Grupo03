{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratório Elastic Search teste com entidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "# Código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to ./nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to ./nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import warnings\n",
    "import subprocess\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import OrderedDict\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import sklearn\n",
    "\n",
    "# Download localmente\n",
    "nltk.data.path.append('./nltk_data')\n",
    "nltk.download('punkt_tab', download_dir='./nltk_data')\n",
    "nltk.download('stopwords',download_dir='./nltk_data')\n",
    "\n",
    "import spacy\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following on terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/python: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "uv run python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo systemctl start docker.service && docker compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 1: Baixar dados do Lupa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# Base de dados de notícias da Lupa\n",
    "url = \"https://docs.google.com/uc?export=download&confirm=t&id=1W067Md2EbvVzW1ufzFg17Hf7Y9cCZxxr\"\n",
    "filename = \"articles_lupa_lab_elasticsearch.zip\"\n",
    "data_path = \"data\"\n",
    "zip_file_path = f\"{data_path}/{filename}\"\n",
    "\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "# Baixa o zip\n",
    "with open(zip_file_path, \"wb\") as f:\n",
    "    f.write(requests.get(url, allow_redirects=True).content)\n",
    "\n",
    "# Extrai o csv do zip\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_path)\n",
    "    \n",
    "output_file = f\"{data_path}/articles_lupa.csv\"\n",
    "assert os.path.exists(output_file)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 2: Pré-processar os dados e gerar embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementações de pré-processamentos de texto. Modifiquem, adicionem, removam conforme necessário.\n",
    "class Preprocessors:\n",
    "    STOPWORDS = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.spacy_nlp = spacy.load(\"pt_core_news_sm\") # Utiliza para lematização\n",
    "        \n",
    "    # Remove stopwords do português\n",
    "    def remove_stopwords(self, text):\n",
    "        # Tokeniza as palavras\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove as stop words\n",
    "        tokens = [word for word in tokens if word not in self.STOPWORDS]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    # Realiza a lematização\n",
    "    def lemma(self, text):\n",
    "        return \" \".join([token.lemma_ for token in self.spacy_nlp(text)])\n",
    "    \n",
    "    # Realiza a stemização\n",
    "    def porter_stemmer(self, text):\n",
    "        # Tokeniza as palavras\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        for index in range(len(tokens)):\n",
    "            # Realiza a stemização\n",
    "            stem_word = self.stemmer.stem(tokens[index])\n",
    "            tokens[index] = stem_word\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    # Transforma o texto em lower case\n",
    "    def lower_case(self, str):\n",
    "        return str.lower()\n",
    "\n",
    "    # Remove urls com regex\n",
    "    def remove_urls(self, text):\n",
    "        url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "        without_urls = re.sub(pattern=url_pattern, repl=' ', string=text)\n",
    "        return without_urls\n",
    "\n",
    "    # Remove números com regex\n",
    "    def remove_numbers(self, text):\n",
    "        number_pattern = r'\\d+'\n",
    "        without_number = re.sub(pattern=number_pattern,\n",
    "    repl=\" \", string=text)\n",
    "        return without_number\n",
    "\n",
    "    # Converte caracteres acentuados para sua versão ASCII\n",
    "    def accented_to_ascii(self, text):\n",
    "        text = unidecode.unidecode(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de palavras que o spacy confunde com entidades (blacklist)\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "def extrair_features(texto):\n",
    "    \"\"\"Extrai entidades que causam confusão\"\"\"\n",
    "    doc = nlp(str(texto))\n",
    "    features = {\"PER\": [], \"ORG\": [], \"LOC\": [], \"NOUNS\": [], \"PROPN\": []}\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        t = ent.text.strip().lower() # Normaliza para comparar\n",
    "        t_original = ent.text.strip()\n",
    "        \n",
    "        # Filtros de qualidade para evitar ruído \n",
    "        if len(t) < 3: continue # Remove \"A\", \"O\"\n",
    "        if t.replace(\"/\", \"\").replace(\"-\", \"\").isdigit(): continue # Remove datas \n",
    "        \n",
    "        if ent.label_ in features:\n",
    "            features[ent.label_].append(t_original)\n",
    "    \n",
    "    for token in doc:\n",
    "        # Ignora pontuação e palavras curtas\n",
    "        if token.is_punct or len(token.text) < 3: continue\n",
    "        \n",
    "        palavra = token.text.strip().lower()\n",
    "        \n",
    "        # Se for substantivo (vacina, autismo, cobrança)\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            features[\"NOUNS\"].append(palavra)\n",
    "            \n",
    "        # Se for nome próprio (Israel, Pix, Covid)\n",
    "        # O spacy transforma coisas que não reconhece em PROPN, então é importante filtrar\n",
    "        elif token.pos_ == \"PROPN\":\n",
    "            features[\"PROPN\"].append(palavra)\n",
    "\n",
    "            \n",
    "    # Retorna listas únicas\n",
    "    return {k: list(set(v)) for k, v in features.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando Embeddings e Extraindo Entidades... (Isso pode demorar um pouco)\n",
      "Geração de embeddings finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Carregar o modelo gerador de embeddings\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Caminho para salvar o dataframe de notícias\n",
    "data_df_path = \"data/data_df.pkl\"\n",
    "\n",
    "# Selecione diferentes pré-processamentos\n",
    "# Exemplo:\n",
    "\n",
    "preprocessor = Preprocessors()\n",
    "preprocessing_steps = [\n",
    "    preprocessor.remove_urls,\n",
    "    preprocessor.remove_stopwords,\n",
    "    preprocessor.remove_numbers,\n",
    "    preprocessor.lemma,\n",
    "    preprocessor.accented_to_ascii,\n",
    "    preprocessor.lower_case,\n",
    "    #preprocessor.porter_stemmer\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "RECREATE_DF = True\n",
    "\n",
    "# Cria o data frame se ele já existir ou se a variável RECREATE_INDEX for verdadeira\n",
    "# Ou (exclusivo) carrega o dataframe salvo\n",
    "if not os.path.exists(data_df_path) or RECREATE_DF:    \n",
    "    df = pd.read_csv(output_file, sep=\";\")[[\"Título\", \"Texto\", \"Data de Publicação\"]]\n",
    "    df[\"Data de Publicação\"] = df[\"Data de Publicação\"].apply(lambda str_date: datetime.strptime(str_date.split(\" - \")[0], \"%d.%m.%Y\"))\n",
    "    df.sort_values(\"Data de Publicação\", inplace=True, ascending=False)\n",
    "\n",
    "    df = df.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    df[\"Embeddings\"] = [None] * len(df)\n",
    "    df[\"doc_id\"] = df.reset_index(drop=True).index\n",
    "    df[\"entidades_per\"] = [[] for _ in range(len(df))]\n",
    "    df[\"entidades_org\"] = [[] for _ in range(len(df))]\n",
    "    df[\"entidades_loc\"] = [[] for _ in range(len(df))]\n",
    "    df[\"entidades_noun\"] = [[] for _ in range(len(df))]\n",
    "    df[\"entidades_propn\"] = [[] for _ in range(len(df))]\n",
    "    \n",
    "    print(\"Gerando Embeddings e Extraindo Entidades... (Isso pode demorar um pouco)\")\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        texto_completo = row[\"Texto\"].strip() + \"\\n\" + row[\"Título\"].strip()\n",
    "\n",
    "        ents = extrair_features(texto_completo)\n",
    "        df.at[i, \"entidades_per\"] = ents[\"PER\"]\n",
    "        df.at[i, \"entidades_org\"] = ents[\"ORG\"]\n",
    "        df.at[i, \"entidades_loc\"] = ents[\"LOC\"]\n",
    "        df.at[i, \"entidades_noun\"] = ents[\"NOUNS\"]\n",
    "        df.at[i, \"entidades_propn\"] = ents[\"PROPN\"]\n",
    "        \n",
    "        df.at[i, \"Texto completo\"] = texto_completo\n",
    "        texto_processado = texto_completo\n",
    "        for preprocessing_step in preprocessing_steps:\n",
    "            texto_processado = preprocessing_step(texto_processado)\n",
    "        \n",
    "        df.at[i, \"Texto processado\"] = texto_processado\n",
    "        embeddings = model.encode(texto_completo).tolist()\n",
    "        df.at[i, \"Embeddings\"] = embeddings\n",
    "        \n",
    "    print(\"Geração de embeddings finalizada.\")\n",
    "    \n",
    "    with open(data_df_path, \"wb\") as f:\n",
    "        df.to_pickle(f)\n",
    "else:\n",
    "    with open(data_df_path, \"rb\") as f:\n",
    "        df = pd.read_pickle(f)\n",
    "    print(\"Dataframe carregado de arquivo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 3: Indexar dados no ElasticSearch (Lembrem-se de reindexar os dados se os pré-processamentos mudarem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    hosts = [{'host': \"localhost\", 'port': 9200, \"scheme\": \"https\"}],\n",
    "    basic_auth=(\"elastic\",\"elastic\"),\n",
    "    verify_certs = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice 'verificacoes_lupa' criado.\n",
      "Índice preenchido.\n",
      "Indexação finalizada.\n"
     ]
    }
   ],
   "source": [
    "RECREATE_INDEX = True\n",
    "\n",
    "index_name = \"verificacoes_lupa\"\n",
    "\n",
    "# Se a flag for True e se o índice existir, ele é deletado\n",
    "if RECREATE_INDEX and es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "    print(f\"Índice '{index_name}' deletado.\")\n",
    "\n",
    "# Cria o índice e popula com os dados\n",
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(index=index_name, mappings={\n",
    "        \"properties\": {\n",
    "            \"doc_id\": {\"type\": \"integer\"},\n",
    "            \"full_text\": {\"type\": \"text\"},\n",
    "            \"processed_text\": {\"type\": \"text\"},\n",
    "            \"embeddings\": {\"type\": \"dense_vector\", \"dims\": 384},\n",
    "            \"entidades_per\": {\"type\": \"keyword\"},\n",
    "            \"entidades_org\": {\"type\": \"keyword\"},\n",
    "            \"entidades_loc\": {\"type\": \"keyword\"},\n",
    "            \"entidades_noun\": {\"type\": \"keyword\"},\n",
    "            \"entidades_propn\": {\"type\": \"keyword\"},\n",
    "        }\n",
    "    })\n",
    "    print(f\"Índice '{index_name}' criado.\")\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        es.index(index=index_name, id=row[\"doc_id\"], body={\n",
    "            \"doc_id\": row[\"doc_id\"],\n",
    "            \"full_text\": row[\"Texto completo\"],\n",
    "            \"processed_text\": row[\"Texto processado\"],\n",
    "            \"embeddings\": row[\"Embeddings\"],\n",
    "            \"entidades_per\": row[\"entidades_per\"],\n",
    "            \"entidades_org\": row[\"entidades_org\"],\n",
    "            \"entidades_loc\": row[\"entidades_loc\"],\n",
    "            \"entidades_noun\": row[\"entidades_noun\"],\n",
    "            \"entidades_propn\": row[\"entidades_propn\"],\n",
    "        })\n",
    "    print(\"Índice preenchido.\")\n",
    "\n",
    "print(\"Indexação finalizada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estas serão as queries QF1 e QF2\n",
    "with open(\"data/queries_fixadas.txt\", \"r\") as f:\n",
    "    queries_fixadas = [line.strip() for line in f.readlines()]\n",
    "    assert len(queries_fixadas) == 2\n",
    "    QF1 = queries_fixadas[0]\n",
    "    QF2 = queries_fixadas[1]\n",
    "    \n",
    "# Preencha aqui as queries do grupo\n",
    "QP1 = \"vacina causa autismo\"\n",
    "QP2 = \"massacre em Israel\"\n",
    "\n",
    "queries = OrderedDict()\n",
    "queries[\"QF1\"] = QF1\n",
    "queries[\"QF2\"] = QF2\n",
    "queries[\"QP1\"] = QP1\n",
    "queries[\"QP2\"] = QP2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Busca Léxica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação de busca esparsa (léxica) com BM25\n",
    "def lexical_search(queries: dict[str, str]):\n",
    "    lexical_results = {}\n",
    "    for query_id, query in queries.items():\n",
    "        \n",
    "        # Pré-processa os dados\n",
    "        for preprocessing_step in preprocessing_steps:\n",
    "            query = preprocessing_step(query)\n",
    "        \n",
    "        search_query = {\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"processed_text\": query\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Realiza a busca\n",
    "        response = es.search(index=index_name, body=search_query)\n",
    "        \n",
    "        hits_results = []\n",
    "        # Recupera os resultados\n",
    "        for hit in response[\"hits\"][\"hits\"]:\n",
    "            hits_results.append((hit[\"_source\"][\"doc_id\"], hit[\"_score\"]))\n",
    "        lexical_results[query_id] = hits_results\n",
    "        \n",
    "    return lexical_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Busca Semântica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza busca semântica (densa) com KNN exato\n",
    "def semantic_search(queries: dict[str, str]):\n",
    "    semantic_results = {}\n",
    "    \n",
    "    for query_id, query in queries.items():\n",
    "        # Aplica todos os pré-processamentos aos dados\n",
    "        for preprocessing_step in preprocessing_steps:\n",
    "            query = preprocessing_step(query)\n",
    "            \n",
    "        query_vector = model.encode(query).tolist()\n",
    "        \n",
    "        \n",
    "        search_query = {\n",
    "            \"query\": {\n",
    "                \"script_score\": {\n",
    "                    \"query\": {\"match_all\": {}},\n",
    "                    \"script\": {\n",
    "                        \"source\": \"cosineSimilarity(params.query_vector, 'embeddings') + 1.0\",\n",
    "                        \"params\": {\"query_vector\": query_vector}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Realiza a busca\n",
    "        response = es.search(index=index_name, body=search_query)\n",
    "        \n",
    "        hits_results = []\n",
    "        # Recupera top 10 resultados\n",
    "        for hit in response[\"hits\"][\"hits\"]:\n",
    "            hits_results.append((hit[\"_source\"][\"doc_id\"], hit[\"_score\"]))\n",
    "            \n",
    "        semantic_results[query_id] = hits_results\n",
    "\n",
    "    return semantic_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Busca Híbrida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca híbrida ou RRF. Implemente sua solução aqui. Você pode realizar as duas buscas anteriores (léxica e semântica) como base para formar a busca híbrida.\n",
    "\n",
    "def hybrid_search_v1(queries: dict[str, str]):\n",
    "    lexical_results = lexical_search(queries)\n",
    "    semantic_results = semantic_search(queries)\n",
    "\n",
    "    print(lexical_results)\n",
    "    print(semantic_results)\n",
    "\n",
    "    alpha = 0.8\n",
    "    beta = 0.2\n",
    "    k_rrf = 60\n",
    "\n",
    "    relevance_documents = {}\n",
    "\n",
    "    for query in queries.keys():\n",
    "\n",
    "        lexical_rank = {}\n",
    "        semantic_rank = {}\n",
    "\n",
    "        for i, (doc_id, _) in enumerate(lexical_results[query]):\n",
    "            lexical_rank[doc_id] = i + 1\n",
    "\n",
    "        for i, (doc_id, _) in enumerate(semantic_results[query]):\n",
    "            semantic_rank[doc_id] = i + 1\n",
    "\n",
    "        all_docs = set(lexical_rank).union(set(semantic_rank))\n",
    "\n",
    "        relevance_documents[query] = []\n",
    "\n",
    "        for doc_id in all_docs:\n",
    "            rrf_lex = 0\n",
    "            rrf_sem = 0\n",
    "\n",
    "            if doc_id in lexical_rank:\n",
    "                rrf_lex = 1 / (k_rrf + lexical_rank[doc_id])\n",
    "\n",
    "            if doc_id in semantic_rank:\n",
    "                rrf_sem = 1 / (k_rrf + semantic_rank[doc_id])\n",
    "\n",
    "            score = rrf_lex +  rrf_sem\n",
    "            relevance_documents[query].append((doc_id, score))\n",
    "\n",
    "        relevance_documents[query].sort(key=lambda x: x[1], reverse=True)\n",
    "        relevance_documents[query] = relevance_documents[query][:10]\n",
    "\n",
    "    for doc_id, score in relevance_documents[\"QF1\"]:\n",
    "        print(f\"DOC_ID={doc_id} SCORE={score}\")\n",
    "   \n",
    "    return relevance_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca híbrida ou RRF. Implemente sua solução aqui. Você pode realizar as duas buscas anteriores (léxica e semântica) como base para formar a busca híbrida.\n",
    "def hybrid_search_v2(queries: dict[str, str]):\n",
    "    lexical_results = lexical_search(queries)\n",
    "    semantic_results = semantic_search(queries)\n",
    "\n",
    "    print(lexical_results)\n",
    "    print(semantic_results)\n",
    "\n",
    "    alpha = 0.8\n",
    "    beta = 0.2\n",
    "    k_rrf = 60\n",
    "\n",
    "    relevance_documents = {}\n",
    "\n",
    "    for query in queries.keys():\n",
    "\n",
    "        lexical_rank = {}\n",
    "        semantic_rank = {}\n",
    "\n",
    "        for i, (doc_id, _) in enumerate(lexical_results[query]):\n",
    "            lexical_rank[doc_id] = i + 1\n",
    "\n",
    "        for i, (doc_id, _) in enumerate(semantic_results[query]):\n",
    "            semantic_rank[doc_id] = i + 1\n",
    "\n",
    "        all_docs = set(lexical_rank).union(set(semantic_rank))\n",
    "\n",
    "        relevance_documents[query] = []\n",
    "\n",
    "        for doc_id in all_docs:\n",
    "            rrf_lex = 0\n",
    "            rrf_sem = 0\n",
    "\n",
    "            if doc_id in lexical_rank:\n",
    "                rrf_lex = 1 / (k_rrf + lexical_rank[doc_id])\n",
    "\n",
    "            if doc_id in semantic_rank:\n",
    "                rrf_sem = 1 / (k_rrf + semantic_rank[doc_id])\n",
    "\n",
    "            score = rrf_lex +  rrf_sem\n",
    "            relevance_documents[query].append((doc_id, score))\n",
    "\n",
    "        relevance_documents[query].sort(key=lambda x: x[1], reverse=True)\n",
    "        relevance_documents[query] = relevance_documents[query][:10]\n",
    "\n",
    "    for doc_id, score in relevance_documents[\"QF1\"]:\n",
    "        print(f\"DOC_ID={doc_id} SCORE={score}\")\n",
    "   \n",
    "    return relevance_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Busca Criativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemente sua própria estratégia de busca, podendo ela ser esparsa, densa ou híbrida. Implemente algo como \"more_like_this\", \"BM35\", \"fuzzy\" etc.\n",
    "def creative_search_v1(queries: dict[str, str]):\n",
    "    # 1. Extrai entidades das consultas do usuário\n",
    "    resultados_queries = {}\n",
    "\n",
    "    for query_id, query in queries.items():\n",
    "        ents_query = extrair_features(query)\n",
    "        print(f\"Entidades da {query_id}, {query}\", ents_query)\n",
    "        q_per = ents_query[\"PER\"]\n",
    "        q_org = ents_query[\"ORG\"]\n",
    "        q_loc = ents_query[\"LOC\"]\n",
    "        q_noun = ents_query[\"NOUNS\"]\n",
    "        q_propn = ents_query[\"PROPN\"]\n",
    "        for processor in preprocessing_steps:\n",
    "            query = processor(query)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Gera vetor da query\n",
    "        query_vector = model.encode(query).tolist()\n",
    "        \n",
    "        # 2. Monta a query usando uma combinação de BM25, KNN e boost por entidades\n",
    "        body = {\n",
    "            \"size\": 10,\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    # \"Must\" garante que o assunto seja relevante (busca híbrida simples)\n",
    "                    \"must\": [\n",
    "                        {\n",
    "                            \"bool\": {\n",
    "                                \"should\": [\n",
    "                                    {\"match\": {\"processed_text\": query}}, # BM25\n",
    "                                    {\"knn\": {\"field\": \"embeddings\", \"query_vector\": query_vector, \"k\": 10, \"boost\": 0.5}} # Vetorial\n",
    "                                ]\n",
    "                            }\n",
    "                        }\n",
    "                    ],\n",
    "                    \n",
    "                    # \"Should\" é o bônus da busca criativa\n",
    "                    # Se bater a entidade, a nota sobe.\n",
    "                    \"should\": [\n",
    "                        {\"terms\": {\"entidades_per\": q_per, \"boost\": 5.0}},\n",
    "                        {\"terms\": {\"entidades_org\": q_org, \"boost\": 5.0}},\n",
    "                        {\"terms\": {\"entidades_loc\": q_loc, \"boost\": 4.0}},\n",
    "\n",
    "                        {\"terms\": {\"entidades_noun\": q_noun, \"boost\": 4.0}},\n",
    "                        {\"terms\": {\"entidades_propn\": q_propn, \"boost\": 3.5}},\n",
    "\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Executa a busca\n",
    "        res = es.search(index=index_name, body=body)\n",
    "        \n",
    "        # Formata retorno apenas com doc_id e score\n",
    "        resultados = []\n",
    "        for hit in res['hits']['hits']:\n",
    "            resultados.append((hit['_source']['doc_id'], hit[\"_score\"]))\n",
    "        \n",
    "        resultados_queries[query_id] = resultados\n",
    "        \n",
    "    return resultados_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemente sua própria estratégia de busca, podendo ela ser esparsa, densa ou híbrida. Implemente algo como \"more_like_this\", \"BM35\", \"fuzzy\" etc.\n",
    "def creative_search_v2(queries: dict[str, str]):\n",
    "    # 1. Extrai entidades das consultas do usuário\n",
    "    resultados_queries = {}\n",
    "\n",
    "    for query_id, query in queries.items():\n",
    "        ents_query = extrair_features(query)\n",
    "        print(f\"Entidades da {query_id}, {query}\", ents_query)\n",
    "        q_per   = ents_query[\"PER\"]\n",
    "        q_org   = ents_query[\"ORG\"]\n",
    "        q_loc   = ents_query[\"LOC\"]\n",
    "        q_noun  = ents_query[\"NOUNS\"]\n",
    "        q_propn = ents_query[\"PROPN\"]\n",
    "        \n",
    "        for processor in preprocessing_steps:\n",
    "            query = processor(query)\n",
    "\n",
    "        \n",
    "        # Gera vetor da query\n",
    "        query_vector = model.encode(query).tolist()\n",
    "        \n",
    "        # 2. Monta a query usando uma combinação de BM25, KNN e boost por entidades\n",
    "        body = {\n",
    "            \"size\": 10,\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    # \"Must\" garante que o assunto seja relevante (busca híbrida simples)\n",
    "                    \"must\": [{\n",
    "                            \"bool\": {\n",
    "                                \"should\": [\n",
    "                                    {\"match\": {\"processed_text\": query}}, # BM25\n",
    "                                    {\"knn\": {\"field\": \"embeddings\", \"query_vector\": query_vector, \"k\": 10, \"boost\": 0.5}} # Vetorial\n",
    "                            ]}\n",
    "                    }],\n",
    "                    \n",
    "                    # \"Should\" é o bônus da busca criativa\n",
    "                    # Se bater a entidade, a nota sobe.\n",
    "                    \"should\": [\n",
    "                        {\"terms\": {\"entidades_per\": q_per, \"boost\": 5.0}},\n",
    "                        {\"terms\": {\"entidades_org\": q_org, \"boost\": 5.0}},\n",
    "                        {\"terms\": {\"entidades_loc\": q_loc, \"boost\": 4.0}},\n",
    "\n",
    "                        {\"terms\": {\"entidades_noun\": q_noun, \"boost\": 4.0}},\n",
    "                        {\"terms\": {\"entidades_propn\": q_propn, \"boost\": 3.5}},\n",
    "\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Executa a busca\n",
    "        res = es.search(index=index_name, body=body)\n",
    "        \n",
    "        # Formata retorno apenas com doc_id e score \n",
    "        resultados = []\n",
    "        for hit in res['hits']['hits']:\n",
    "            resultados.append((hit['_source']['doc_id'], hit[\"_score\"]))\n",
    "        \n",
    "        resultados_queries[query_id] = resultados\n",
    "        \n",
    "    return resultados_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execução das buscas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'QF1': [(1429, 16.823084), (299, 16.192177), (140, 16.191858), (1689, 15.490684), (2328, 14.387486), (1398, 14.052032), (1735, 13.307976), (410, 12.483434), (1887, 12.423523), (2021, 12.145069)], 'QF2': [(1594, 12.996189), (1237, 12.85702), (1621, 12.774794), (316, 12.742256), (2208, 12.732671), (15, 12.723009), (1067, 12.273449), (510, 12.072852), (122, 11.790132), (1793, 11.518058)], 'QP1': [(2314, 11.017654), (303, 10.456556), (960, 9.499834), (1394, 9.259333), (2519, 8.568039), (2436, 8.433064), (1892, 8.372154), (1552, 8.238545), (1296, 7.72898), (294, 7.6265297)], 'QP2': [(814, 15.349197), (1388, 13.525953), (180, 12.836807), (1992, 11.048485), (1133, 8.7437725), (1578, 8.4807625), (2484, 7.2963786), (1235, 7.2920065), (2550, 7.282001), (2492, 7.243937)]}\n",
      "{'QF1': [(1398, 1.6246119), (2188, 1.5283167), (1429, 1.5064514), (2328, 1.505476), (2555, 1.4939659), (333, 1.4828167), (177, 1.4721812), (410, 1.47214), (2410, 1.4565319), (1125, 1.4546205)], 'QF2': [(1237, 1.8076564), (1722, 1.7621573), (1989, 1.7520217), (1621, 1.7430636), (510, 1.7271051), (1752, 1.7106292), (1132, 1.7047325), (2073, 1.7021568), (861, 1.701055), (1594, 1.7007091)], 'QP1': [(1237, 1.5580077), (359, 1.5565056), (2519, 1.5390477), (186, 1.5387578), (137, 1.530966), (1752, 1.519625), (121, 1.5192797), (1594, 1.5181099), (2498, 1.516624), (1027, 1.514234)], 'QP2': [(814, 1.7237177), (1693, 1.6455504), (1023, 1.6314373), (2123, 1.6291357), (180, 1.6178629), (1388, 1.6123213), (1378, 1.6045778), (304, 1.590568), (1790, 1.5806785), (2566, 1.5670909)]}\n",
      "DOC_ID=1429 SCORE=0.032266458495966696\n",
      "DOC_ID=1398 SCORE=0.031544957774465976\n",
      "DOC_ID=2328 SCORE=0.031009615384615385\n",
      "DOC_ID=410 SCORE=0.029411764705882353\n",
      "DOC_ID=2188 SCORE=0.016129032258064516\n",
      "DOC_ID=299 SCORE=0.016129032258064516\n",
      "DOC_ID=140 SCORE=0.015873015873015872\n",
      "DOC_ID=1689 SCORE=0.015625\n",
      "DOC_ID=2555 SCORE=0.015384615384615385\n",
      "DOC_ID=333 SCORE=0.015151515151515152\n",
      "Entidades da QF1, Haverá cobrança na transferência bancária digital a partir de agora {'PER': [], 'ORG': [], 'LOC': [], 'NOUNS': ['transferência', 'partir', 'cobrança'], 'PROPN': []}\n",
      "Entidades da QF2, As vacinas da covid19 causaram mortes {'PER': [], 'ORG': [], 'LOC': [], 'NOUNS': ['mortes', 'covid19', 'vacinas'], 'PROPN': []}\n",
      "Entidades da QP1, vacina causa autismo {'PER': [], 'ORG': [], 'LOC': [], 'NOUNS': ['vacina', 'causa'], 'PROPN': []}\n",
      "Entidades da QP2, massacre em Israel {'PER': [], 'ORG': [], 'LOC': ['Israel'], 'NOUNS': ['massacre'], 'PROPN': ['israel']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lexical': {'QF1': [(1429, 16.823084),\n",
       "   (299, 16.192177),\n",
       "   (140, 16.191858),\n",
       "   (1689, 15.490684),\n",
       "   (2328, 14.387486),\n",
       "   (1398, 14.052032),\n",
       "   (1735, 13.307976),\n",
       "   (410, 12.483434),\n",
       "   (1887, 12.423523),\n",
       "   (2021, 12.145069)],\n",
       "  'QF2': [(1594, 12.996189),\n",
       "   (1237, 12.85702),\n",
       "   (1621, 12.774794),\n",
       "   (316, 12.742256),\n",
       "   (2208, 12.732671),\n",
       "   (15, 12.723009),\n",
       "   (1067, 12.273449),\n",
       "   (510, 12.072852),\n",
       "   (122, 11.790132),\n",
       "   (1793, 11.518058)],\n",
       "  'QP1': [(2314, 11.017654),\n",
       "   (303, 10.456556),\n",
       "   (960, 9.499834),\n",
       "   (1394, 9.259333),\n",
       "   (2519, 8.568039),\n",
       "   (2436, 8.433064),\n",
       "   (1892, 8.372154),\n",
       "   (1552, 8.238545),\n",
       "   (1296, 7.72898),\n",
       "   (294, 7.6265297)],\n",
       "  'QP2': [(814, 15.349197),\n",
       "   (1388, 13.525953),\n",
       "   (180, 12.836807),\n",
       "   (1992, 11.048485),\n",
       "   (1133, 8.7437725),\n",
       "   (1578, 8.4807625),\n",
       "   (2484, 7.2963786),\n",
       "   (1235, 7.2920065),\n",
       "   (2550, 7.282001),\n",
       "   (2492, 7.243937)]},\n",
       " 'semantic': {'QF1': [(1398, 1.6246119),\n",
       "   (2188, 1.5283167),\n",
       "   (1429, 1.5064514),\n",
       "   (2328, 1.505476),\n",
       "   (2555, 1.4939659),\n",
       "   (333, 1.4828167),\n",
       "   (177, 1.4721812),\n",
       "   (410, 1.47214),\n",
       "   (2410, 1.4565319),\n",
       "   (1125, 1.4546205)],\n",
       "  'QF2': [(1237, 1.8076564),\n",
       "   (1722, 1.7621573),\n",
       "   (1989, 1.7520217),\n",
       "   (1621, 1.7430636),\n",
       "   (510, 1.7271051),\n",
       "   (1752, 1.7106292),\n",
       "   (1132, 1.7047325),\n",
       "   (2073, 1.7021568),\n",
       "   (861, 1.701055),\n",
       "   (1594, 1.7007091)],\n",
       "  'QP1': [(1237, 1.5580077),\n",
       "   (359, 1.5565056),\n",
       "   (2519, 1.5390477),\n",
       "   (186, 1.5387578),\n",
       "   (137, 1.530966),\n",
       "   (1752, 1.519625),\n",
       "   (121, 1.5192797),\n",
       "   (1594, 1.5181099),\n",
       "   (2498, 1.516624),\n",
       "   (1027, 1.514234)],\n",
       "  'QP2': [(814, 1.7237177),\n",
       "   (1693, 1.6455504),\n",
       "   (1023, 1.6314373),\n",
       "   (2123, 1.6291357),\n",
       "   (180, 1.6178629),\n",
       "   (1388, 1.6123213),\n",
       "   (1378, 1.6045778),\n",
       "   (304, 1.590568),\n",
       "   (1790, 1.5806785),\n",
       "   (2566, 1.5670909)]},\n",
       " 'hybrid': {'QF1': [(1429, 0.032266458495966696),\n",
       "   (1398, 0.031544957774465976),\n",
       "   (2328, 0.031009615384615385),\n",
       "   (410, 0.029411764705882353),\n",
       "   (2188, 0.016129032258064516),\n",
       "   (299, 0.016129032258064516),\n",
       "   (140, 0.015873015873015872),\n",
       "   (1689, 0.015625),\n",
       "   (2555, 0.015384615384615385),\n",
       "   (333, 0.015151515151515152)],\n",
       "  'QF2': [(1237, 0.03252247488101534),\n",
       "   (1621, 0.03149801587301587),\n",
       "   (1594, 0.030679156908665108),\n",
       "   (510, 0.030090497737556562),\n",
       "   (1722, 0.016129032258064516),\n",
       "   (1989, 0.015873015873015872),\n",
       "   (316, 0.015625),\n",
       "   (2208, 0.015384615384615385),\n",
       "   (15, 0.015151515151515152),\n",
       "   (1752, 0.015151515151515152)],\n",
       "  'QP1': [(2519, 0.03125763125763126),\n",
       "   (2314, 0.01639344262295082),\n",
       "   (1237, 0.01639344262295082),\n",
       "   (359, 0.016129032258064516),\n",
       "   (303, 0.016129032258064516),\n",
       "   (960, 0.015873015873015872),\n",
       "   (186, 0.015625),\n",
       "   (1394, 0.015625),\n",
       "   (137, 0.015384615384615385),\n",
       "   (2436, 0.015151515151515152)],\n",
       "  'QP2': [(814, 0.03278688524590164),\n",
       "   (1388, 0.03128054740957967),\n",
       "   (180, 0.03125763125763126),\n",
       "   (1693, 0.016129032258064516),\n",
       "   (1023, 0.015873015873015872),\n",
       "   (1992, 0.015625),\n",
       "   (2123, 0.015625),\n",
       "   (1133, 0.015384615384615385),\n",
       "   (1578, 0.015151515151515152),\n",
       "   (1378, 0.014925373134328358)]},\n",
       " 'creative': {'QF1': [(1429, 20.199831),\n",
       "   (299, 19.192177),\n",
       "   (1689, 18.490685),\n",
       "   (2328, 17.764683),\n",
       "   (1735, 16.307976),\n",
       "   (140, 16.191858),\n",
       "   (1887, 15.423523),\n",
       "   (1834, 14.6868105),\n",
       "   (1398, 14.457799),\n",
       "   (2477, 14.374478)],\n",
       "  'QF2': [(1594, 16.420956),\n",
       "   (1237, 16.308788),\n",
       "   (1621, 16.210669),\n",
       "   (316, 15.742256),\n",
       "   (2208, 15.732671),\n",
       "   (15, 15.723009),\n",
       "   (510, 15.504647),\n",
       "   (1067, 15.273449),\n",
       "   (861, 14.857728),\n",
       "   (122, 14.790132)],\n",
       "  'QP1': [(2314, 14.017654),\n",
       "   (303, 13.456556),\n",
       "   (960, 12.499834),\n",
       "   (1394, 12.259333),\n",
       "   (2519, 11.954379),\n",
       "   (2436, 11.433064),\n",
       "   (1892, 11.372154),\n",
       "   (1552, 11.238545),\n",
       "   (121, 10.742735),\n",
       "   (1296, 10.72898)],\n",
       "  'QP2': [(814, 24.78182),\n",
       "   (1992, 20.048485),\n",
       "   (1388, 19.930756),\n",
       "   (180, 19.242138),\n",
       "   (2492, 13.636333),\n",
       "   (2566, 13.432309),\n",
       "   (304, 13.402611),\n",
       "   (2123, 13.347073),\n",
       "   (1790, 13.335808),\n",
       "   (1023, 13.303265)]}}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_functions = [\n",
    "    (\"lexical\", lexical_search),\n",
    "    (\"semantic\", semantic_search),\n",
    "    (\"hybrid_v1\", hybrid_search_v1),\n",
    "    (\"hybrid_v2\", hybrid_search_v2),\n",
    "    (\"creative_v1\", creative_search_v1),\n",
    "    (\"creative_v2\", creative_search_v2)\n",
    "]\n",
    "\n",
    "def run_all_searches(queries: dict[str, str]):\n",
    "    all_search_results = {}\n",
    "    for search_name, search_function in search_functions:\n",
    "        results = search_function(queries)\n",
    "        all_search_results[search_name] = results\n",
    "    return all_search_results\n",
    "\n",
    "run_all_searches(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analise os resultados da busca e aprimore a busca!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'QF1': [(1429, 16.823084), (299, 16.192177), (140, 16.191858), (1689, 15.490684), (2328, 14.387486), (1398, 14.052032), (1735, 13.307976), (410, 12.483434), (1887, 12.423523), (2021, 12.145069)], 'QF2': [(1594, 12.996189), (1237, 12.85702), (1621, 12.774794), (316, 12.742256), (2208, 12.732671), (15, 12.723009), (1067, 12.273449), (510, 12.072852), (122, 11.790132), (1793, 11.518058)], 'QP1': [(2314, 11.017654), (303, 10.456556), (960, 9.499834), (1394, 9.259333), (2519, 8.568039), (2436, 8.433064), (1892, 8.372154), (1552, 8.238545), (1296, 7.72898), (294, 7.6265297)], 'QP2': [(814, 15.349197), (1388, 13.525953), (180, 12.836807), (1992, 11.048485), (1133, 8.7437725), (1578, 8.4807625), (2484, 7.2963786), (1235, 7.2920065), (2550, 7.282001), (2492, 7.243937)]}\n",
      "{'QF1': [(1398, 1.6246119), (2188, 1.5283167), (1429, 1.5064514), (2328, 1.505476), (2555, 1.4939659), (333, 1.4828167), (177, 1.4721812), (410, 1.47214), (2410, 1.4565319), (1125, 1.4546205)], 'QF2': [(1237, 1.8076564), (1722, 1.7621573), (1989, 1.7520217), (1621, 1.7430636), (510, 1.7271051), (1752, 1.7106292), (1132, 1.7047325), (2073, 1.7021568), (861, 1.701055), (1594, 1.7007091)], 'QP1': [(1237, 1.5580077), (359, 1.5565056), (2519, 1.5390477), (186, 1.5387578), (137, 1.530966), (1752, 1.519625), (121, 1.5192797), (1594, 1.5181099), (2498, 1.516624), (1027, 1.514234)], 'QP2': [(814, 1.7237177), (1693, 1.6455504), (1023, 1.6314373), (2123, 1.6291357), (180, 1.6178629), (1388, 1.6123213), (1378, 1.6045778), (304, 1.590568), (1790, 1.5806785), (2566, 1.5670909)]}\n",
      "DOC_ID=1429 SCORE=0.032266458495966696\n",
      "DOC_ID=1398 SCORE=0.031544957774465976\n",
      "DOC_ID=2328 SCORE=0.031009615384615385\n",
      "DOC_ID=410 SCORE=0.029411764705882353\n",
      "DOC_ID=2188 SCORE=0.016129032258064516\n",
      "DOC_ID=299 SCORE=0.016129032258064516\n",
      "DOC_ID=140 SCORE=0.015873015873015872\n",
      "DOC_ID=1689 SCORE=0.015625\n",
      "DOC_ID=2555 SCORE=0.015384615384615385\n",
      "DOC_ID=333 SCORE=0.015151515151515152\n",
      "Entidades da QF1, Haverá cobrança na transferência bancária digital a partir de agora {'PER': [], 'ORG': [], 'LOC': [], 'NOUNS': ['transferência', 'partir', 'cobrança'], 'PROPN': []}\n",
      "Entidades da QF2, As vacinas da covid19 causaram mortes {'PER': [], 'ORG': [], 'LOC': [], 'NOUNS': ['mortes', 'covid19', 'vacinas'], 'PROPN': []}\n",
      "Entidades da QP1, vacina causa autismo {'PER': [], 'ORG': [], 'LOC': [], 'NOUNS': ['vacina', 'causa'], 'PROPN': []}\n",
      "Entidades da QP2, massacre em Israel {'PER': [], 'ORG': [], 'LOC': ['Israel'], 'NOUNS': ['massacre'], 'PROPN': ['israel']}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexical</th>\n",
       "      <th>semantic</th>\n",
       "      <th>hybrid</th>\n",
       "      <th>creative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>[(1429, 16.823084), (299, 16.192177), (140, 16...</td>\n",
       "      <td>[(1398, 1.6246119), (2188, 1.5283167), (1429, ...</td>\n",
       "      <td>[(1429, 0.032266458495966696), (1398, 0.031544...</td>\n",
       "      <td>[(1429, 20.199831), (299, 19.192177), (1689, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>[(1594, 12.996189), (1237, 12.85702), (1621, 1...</td>\n",
       "      <td>[(1237, 1.8076564), (1722, 1.7621573), (1989, ...</td>\n",
       "      <td>[(1237, 0.03252247488101534), (1621, 0.0314980...</td>\n",
       "      <td>[(1594, 16.420956), (1237, 16.308788), (1621, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>[(2314, 11.017654), (303, 10.456556), (960, 9....</td>\n",
       "      <td>[(1237, 1.5580077), (359, 1.5565056), (2519, 1...</td>\n",
       "      <td>[(2519, 0.03125763125763126), (2314, 0.0163934...</td>\n",
       "      <td>[(2314, 14.017654), (303, 13.456556), (960, 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>[(814, 15.349197), (1388, 13.525953), (180, 12...</td>\n",
       "      <td>[(814, 1.7237177), (1693, 1.6455504), (1023, 1...</td>\n",
       "      <td>[(814, 0.03278688524590164), (1388, 0.03128054...</td>\n",
       "      <td>[(814, 24.78182), (1992, 20.048485), (1388, 19...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               lexical  \\\n",
       "QF1  [(1429, 16.823084), (299, 16.192177), (140, 16...   \n",
       "QF2  [(1594, 12.996189), (1237, 12.85702), (1621, 1...   \n",
       "QP1  [(2314, 11.017654), (303, 10.456556), (960, 9....   \n",
       "QP2  [(814, 15.349197), (1388, 13.525953), (180, 12...   \n",
       "\n",
       "                                              semantic  \\\n",
       "QF1  [(1398, 1.6246119), (2188, 1.5283167), (1429, ...   \n",
       "QF2  [(1237, 1.8076564), (1722, 1.7621573), (1989, ...   \n",
       "QP1  [(1237, 1.5580077), (359, 1.5565056), (2519, 1...   \n",
       "QP2  [(814, 1.7237177), (1693, 1.6455504), (1023, 1...   \n",
       "\n",
       "                                                hybrid  \\\n",
       "QF1  [(1429, 0.032266458495966696), (1398, 0.031544...   \n",
       "QF2  [(1237, 0.03252247488101534), (1621, 0.0314980...   \n",
       "QP1  [(2519, 0.03125763125763126), (2314, 0.0163934...   \n",
       "QP2  [(814, 0.03278688524590164), (1388, 0.03128054...   \n",
       "\n",
       "                                              creative  \n",
       "QF1  [(1429, 20.199831), (299, 19.192177), (1689, 1...  \n",
       "QF2  [(1594, 16.420956), (1237, 16.308788), (1621, ...  \n",
       "QP1  [(2314, 14.017654), (303, 13.456556), (960, 12...  \n",
       "QP2  [(814, 24.78182), (1992, 20.048485), (1388, 19...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_search_results = run_all_searches(queries)\n",
    "search_results_df = pd.DataFrame(all_search_results)\n",
    "search_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados das buscas salvos em 'data/search_results.csv'.\n",
      "Documentos de interesse salvos em 'data/documents.csv'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexical</th>\n",
       "      <th>semantic</th>\n",
       "      <th>hybrid</th>\n",
       "      <th>creative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>1429</td>\n",
       "      <td>1398</td>\n",
       "      <td>1429</td>\n",
       "      <td>1429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>299</td>\n",
       "      <td>2188</td>\n",
       "      <td>1398</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>140</td>\n",
       "      <td>1429</td>\n",
       "      <td>2328</td>\n",
       "      <td>1689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>1689</td>\n",
       "      <td>2328</td>\n",
       "      <td>410</td>\n",
       "      <td>2328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>2328</td>\n",
       "      <td>2555</td>\n",
       "      <td>2188</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>1398</td>\n",
       "      <td>333</td>\n",
       "      <td>299</td>\n",
       "      <td>1735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>1735</td>\n",
       "      <td>177</td>\n",
       "      <td>140</td>\n",
       "      <td>1398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>1689</td>\n",
       "      <td>1887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>1887</td>\n",
       "      <td>2410</td>\n",
       "      <td>2555</td>\n",
       "      <td>1834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>2021</td>\n",
       "      <td>1125</td>\n",
       "      <td>333</td>\n",
       "      <td>2477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>1594</td>\n",
       "      <td>1237</td>\n",
       "      <td>1237</td>\n",
       "      <td>1594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>1237</td>\n",
       "      <td>1722</td>\n",
       "      <td>1621</td>\n",
       "      <td>1237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>1621</td>\n",
       "      <td>1989</td>\n",
       "      <td>1594</td>\n",
       "      <td>1621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>316</td>\n",
       "      <td>1621</td>\n",
       "      <td>510</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>2208</td>\n",
       "      <td>510</td>\n",
       "      <td>1722</td>\n",
       "      <td>2208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>15</td>\n",
       "      <td>1752</td>\n",
       "      <td>1989</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>1067</td>\n",
       "      <td>1132</td>\n",
       "      <td>316</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>510</td>\n",
       "      <td>2073</td>\n",
       "      <td>2208</td>\n",
       "      <td>1067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>122</td>\n",
       "      <td>861</td>\n",
       "      <td>15</td>\n",
       "      <td>861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>1793</td>\n",
       "      <td>1594</td>\n",
       "      <td>1752</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>2314</td>\n",
       "      <td>1237</td>\n",
       "      <td>2519</td>\n",
       "      <td>2314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>303</td>\n",
       "      <td>359</td>\n",
       "      <td>2314</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>960</td>\n",
       "      <td>2519</td>\n",
       "      <td>1237</td>\n",
       "      <td>960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>1394</td>\n",
       "      <td>186</td>\n",
       "      <td>359</td>\n",
       "      <td>1394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>2436</td>\n",
       "      <td>137</td>\n",
       "      <td>303</td>\n",
       "      <td>2519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>1892</td>\n",
       "      <td>1752</td>\n",
       "      <td>960</td>\n",
       "      <td>2436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>1552</td>\n",
       "      <td>121</td>\n",
       "      <td>186</td>\n",
       "      <td>1892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>1296</td>\n",
       "      <td>1594</td>\n",
       "      <td>1394</td>\n",
       "      <td>1552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>294</td>\n",
       "      <td>2498</td>\n",
       "      <td>137</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>121</td>\n",
       "      <td>1027</td>\n",
       "      <td>2436</td>\n",
       "      <td>1296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>814</td>\n",
       "      <td>814</td>\n",
       "      <td>814</td>\n",
       "      <td>814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1388</td>\n",
       "      <td>1693</td>\n",
       "      <td>1388</td>\n",
       "      <td>1388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>180</td>\n",
       "      <td>1023</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1992</td>\n",
       "      <td>2123</td>\n",
       "      <td>1693</td>\n",
       "      <td>1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1133</td>\n",
       "      <td>180</td>\n",
       "      <td>1023</td>\n",
       "      <td>1133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1578</td>\n",
       "      <td>1388</td>\n",
       "      <td>1992</td>\n",
       "      <td>2492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>2484</td>\n",
       "      <td>1378</td>\n",
       "      <td>2123</td>\n",
       "      <td>1578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>1235</td>\n",
       "      <td>304</td>\n",
       "      <td>1133</td>\n",
       "      <td>2566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>2550</td>\n",
       "      <td>1790</td>\n",
       "      <td>1578</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>2492</td>\n",
       "      <td>2566</td>\n",
       "      <td>1378</td>\n",
       "      <td>2123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lexical  semantic  hybrid  creative\n",
       "QF1     1429      1398    1429      1429\n",
       "QF1      299      2188    1398       299\n",
       "QF1      140      1429    2328      1689\n",
       "QF1     1689      2328     410      2328\n",
       "QF1     2328      2555    2188       140\n",
       "QF1     1398       333     299      1735\n",
       "QF1     1735       177     140      1398\n",
       "QF1      410       410    1689      1887\n",
       "QF1     1887      2410    2555      1834\n",
       "QF1     2021      1125     333      2477\n",
       "QF2     1594      1237    1237      1594\n",
       "QF2     1237      1722    1621      1237\n",
       "QF2     1621      1989    1594      1621\n",
       "QF2      316      1621     510       316\n",
       "QF2     2208       510    1722      2208\n",
       "QF2       15      1752    1989        15\n",
       "QF2     1067      1132     316       510\n",
       "QF2      510      2073    2208      1067\n",
       "QF2      122       861      15       861\n",
       "QF2     1793      1594    1752       122\n",
       "QP1     2314      1237    2519      2314\n",
       "QP1      303       359    2314       303\n",
       "QP1      960      2519    1237       960\n",
       "QP1     1394       186     359      1394\n",
       "QP1     2436       137     303      2519\n",
       "QP1     1892      1752     960      2436\n",
       "QP1     1552       121     186      1892\n",
       "QP1     1296      1594    1394      1552\n",
       "QP1      294      2498     137       121\n",
       "QP1      121      1027    2436      1296\n",
       "QP2      814       814     814       814\n",
       "QP2     1388      1693    1388      1388\n",
       "QP2      180      1023     180       180\n",
       "QP2     1992      2123    1693      1992\n",
       "QP2     1133       180    1023      1133\n",
       "QP2     1578      1388    1992      2492\n",
       "QP2     2484      1378    2123      1578\n",
       "QP2     1235       304    1133      2566\n",
       "QP2     2550      1790    1578       304\n",
       "QP2     2492      2566    1378      2123"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_exploded_df(search_results_df):\n",
    "    exploded_search_results_df = pd.concat([search_results_df[col].explode() for col in search_results_df.columns], axis=1)\n",
    "    exploded_search_results_df = exploded_search_results_df.apply(lambda l: [doc_id for doc_id, _ in l])\n",
    "    return exploded_search_results_df\n",
    "\n",
    "def generate_found_docs_text_df(exploded_search_results_df, all_docs_df):\n",
    "    # Recupera os ids únicos dos documentos\n",
    "    documents_ids = set(exploded_search_results_df.to_numpy().flatten().tolist())\n",
    "\n",
    "    # Salva os textos e os ids dos documentos que foram encontrados em usas buscas\n",
    "    documents_df = all_docs_df[all_docs_df[\"doc_id\"].isin(documents_ids)][[\"Texto processado\", \"doc_id\"]]\n",
    "    return documents_df\n",
    "\n",
    "exploded_df = generate_exploded_df(search_results_df)\n",
    "found_docs_text_df = generate_found_docs_text_df(exploded_df, all_docs_df=df)\n",
    "\n",
    "def save_results_to_file(exploded_df: pd.DataFrame,\n",
    "                         found_docs_text_df: pd.DataFrame,\n",
    "                         exploded_df_save_filepath: str = \"data/search_results.csv\",\n",
    "                         found_docs_text_save_filepath: str = \"data/documents.csv\"):\n",
    "    exploded_df.to_csv(exploded_df_save_filepath)\n",
    "    found_docs_text_df.to_csv(found_docs_text_save_filepath)\n",
    "    print(\"Resultados das buscas salvos em 'data/search_results.csv'.\")\n",
    "    print(\"Documentos de interesse salvos em 'data/documents.csv'.\")\n",
    "    \n",
    "save_results_to_file(exploded_df, found_docs_text_df)\n",
    "exploded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>É falso que houve megaprotesto na Alemanha con...</td>\n",
       "      <td>circular rede social foto mostrar dezena carro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Estudo não aponta que carga viral de vacinados...</td>\n",
       "      <td>circular rede social estudo publicar revista c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>É antiga foto de manifestante em cima de carro...</td>\n",
       "      <td>circular rede social foto homem mascarar , seg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Diretor da Anvisa não pediu demissão e critico...</td>\n",
       "      <td>circular rede social diretor anvisa , agencia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Jogadoras da seleção de futebol não posaram in...</td>\n",
       "      <td>circular rede imagem dois jogadora selecao bra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>2565</td>\n",
       "      <td>Es falso que farmacias de Italia estén distrib...</td>\n",
       "      <td>circula en las rede sociales las farmacias en ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2566</th>\n",
       "      <td>2566</td>\n",
       "      <td>Foto viral que mostra Faixa de Gaza após bomba...</td>\n",
       "      <td>circular rede social foto mostrar cidade ceu c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2567</th>\n",
       "      <td>2567</td>\n",
       "      <td>É falso que STF afastou Bolsonaro do controle ...</td>\n",
       "      <td>circular rede social supremo tribunal federal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2568</th>\n",
       "      <td>2568</td>\n",
       "      <td>É falso que Magazine Luiza 'financia' fome dos...</td>\n",
       "      <td>em o comissao parlamentar inquerito ( cpi ) on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2569</th>\n",
       "      <td>2569</td>\n",
       "      <td>Como a nova faixa de isenção impactará no Impo...</td>\n",
       "      <td>em    fevereiro , receita federal anunciar mud...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2570 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      doc_id                                              title  \\\n",
       "0          0  É falso que houve megaprotesto na Alemanha con...   \n",
       "1          1  Estudo não aponta que carga viral de vacinados...   \n",
       "2          2  É antiga foto de manifestante em cima de carro...   \n",
       "3          3  Diretor da Anvisa não pediu demissão e critico...   \n",
       "4          4  Jogadoras da seleção de futebol não posaram in...   \n",
       "...      ...                                                ...   \n",
       "2565    2565  Es falso que farmacias de Italia estén distrib...   \n",
       "2566    2566  Foto viral que mostra Faixa de Gaza após bomba...   \n",
       "2567    2567  É falso que STF afastou Bolsonaro do controle ...   \n",
       "2568    2568  É falso que Magazine Luiza 'financia' fome dos...   \n",
       "2569    2569  Como a nova faixa de isenção impactará no Impo...   \n",
       "\n",
       "                                                content  \n",
       "0     circular rede social foto mostrar dezena carro...  \n",
       "1     circular rede social estudo publicar revista c...  \n",
       "2     circular rede social foto homem mascarar , seg...  \n",
       "3     circular rede social diretor anvisa , agencia ...  \n",
       "4     circular rede imagem dois jogadora selecao bra...  \n",
       "...                                                 ...  \n",
       "2565  circula en las rede sociales las farmacias en ...  \n",
       "2566  circular rede social foto mostrar cidade ceu c...  \n",
       "2567  circular rede social supremo tribunal federal ...  \n",
       "2568  em o comissao parlamentar inquerito ( cpi ) on...  \n",
       "2569  em    fevereiro , receita federal anunciar mud...  \n",
       "\n",
       "[2570 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_id_map(all_docs_df, output_csv_path):\n",
    "    \"\"\"\n",
    "    Exports a stable mapping of ALL documents used in Elasticsearch.\n",
    "\n",
    "    The doc_id values will exactly match those returned by search results.\n",
    "    \"\"\"\n",
    "\n",
    "    required_cols = {\"doc_id\", \"Título\", \"Texto processado\"}\n",
    "    missing = required_cols - set(all_docs_df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    id_map_df = (\n",
    "        all_docs_df[[\"doc_id\", \"Título\", \"Texto processado\"]]\n",
    "        .rename(columns={\n",
    "            \"Título\": \"title\",\n",
    "            \"Texto processado\": \"content\"\n",
    "        })\n",
    "        .sort_values(\"doc_id\")   # optional, but good for reproducibility\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    id_map_df.to_csv(output_csv_path, sep=\";\", index=False)\n",
    "\n",
    "    return id_map_df\n",
    "\n",
    "generate_id_map(df, \"data/id_map.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lab-elastic)",
   "language": "python",
   "name": "lab-elastic-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
