{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratório Elastic Search teste com entidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "# Código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to ./nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to ./nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import warnings\n",
    "import subprocess\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import OrderedDict\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import sklearn\n",
    "\n",
    "# Download localmente\n",
    "nltk.data.path.append('./nltk_data')\n",
    "nltk.download('punkt_tab', download_dir='./nltk_data')\n",
    "nltk.download('stopwords',download_dir='./nltk_data')\n",
    "\n",
    "import spacy\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following blocks on the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/python: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!uv run python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo systemctl start docker.service && docker compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 1: Baixar dados do Lupa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# Base de dados de notícias da Lupa\n",
    "url = \"https://docs.google.com/uc?export=download&confirm=t&id=1W067Md2EbvVzW1ufzFg17Hf7Y9cCZxxr\"\n",
    "filename = \"articles_lupa_lab_elasticsearch.zip\"\n",
    "data_path = \"data\"\n",
    "zip_file_path = f\"{data_path}/{filename}\"\n",
    "\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "# Baixa o zip\n",
    "with open(zip_file_path, \"wb\") as f:\n",
    "    f.write(requests.get(url, allow_redirects=True).content)\n",
    "\n",
    "# Extrai o csv do zip\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_path)\n",
    "    \n",
    "output_file = f\"{data_path}/articles_lupa.csv\"\n",
    "assert os.path.exists(output_file)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 2: Pré-processar os dados e gerar embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementações de pré-processamentos de texto. Modifiquem, adicionem, removam conforme necessário.\n",
    "class Preprocessors:\n",
    "    STOPWORDS = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.spacy_nlp = spacy.load(\"pt_core_news_sm\") # Utiliza para lematização\n",
    "        \n",
    "    # Remove stopwords do português\n",
    "    def remove_stopwords(self, text):\n",
    "        # Tokeniza as palavras\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove as stop words\n",
    "        tokens = [word for word in tokens if word not in self.STOPWORDS]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    # Realiza a lematização\n",
    "    def lemma(self, text):\n",
    "        return \" \".join([token.lemma_ for token in self.spacy_nlp(text)])\n",
    "    \n",
    "    # Realiza a stemização\n",
    "    def porter_stemmer(self, text):\n",
    "        # Tokeniza as palavras\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        for index in range(len(tokens)):\n",
    "            # Realiza a stemização\n",
    "            stem_word = self.stemmer.stem(tokens[index])\n",
    "            tokens[index] = stem_word\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    # Transforma o texto em lower case\n",
    "    def lower_case(self, str):\n",
    "        return str.lower()\n",
    "\n",
    "    # Remove urls com regex\n",
    "    def remove_urls(self, text):\n",
    "        url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "        without_urls = re.sub(pattern=url_pattern, repl=' ', string=text)\n",
    "        return without_urls\n",
    "\n",
    "    # Remove números com regex\n",
    "    def remove_numbers(self, text):\n",
    "        number_pattern = r'\\d+'\n",
    "        without_number = re.sub(pattern=number_pattern,\n",
    "    repl=\" \", string=text)\n",
    "        return without_number\n",
    "\n",
    "    # Converte caracteres acentuados para sua versão ASCII\n",
    "    def accented_to_ascii(self, text):\n",
    "        text = unidecode.unidecode(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de palavras que o modelo Small confunde com entidades (Blacklist)\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "def extrair_features(texto):\n",
    "    \"\"\"Extrai entidades limpando ruídos do modelo Small\"\"\"\n",
    "    doc = nlp(str(texto))\n",
    "    features = {\"PER\": [], \"ORG\": [], \"LOC\": [], \"NOUNS\": [], \"PROPN\": []}\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        t = ent.text.strip().lower() # Normaliza para comparar\n",
    "        t_original = ent.text.strip()\n",
    "        \n",
    "        # Filtros de Qualidade para o Small\n",
    "        if len(t) < 3: continue # Remove \"A\", \"O\"\n",
    "        if t.replace(\"/\", \"\").replace(\"-\", \"\").isdigit(): continue # Remove datas puras\n",
    "        \n",
    "        if ent.label_ in features:\n",
    "            features[ent.label_].append(t_original)\n",
    "    \n",
    "    for token in doc:\n",
    "        # Ignora pontuação e palavras curtas\n",
    "        if token.is_punct or len(token.text) < 3: continue\n",
    "        \n",
    "        palavra = token.text.strip().lower()\n",
    "        \n",
    "        # Se for SUBSTANTIVO (Ex: vacina, autismo, cobrança)\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            features[\"NOUNS\"].append(palavra)\n",
    "            \n",
    "        # Se for NOME PRÓPRIO (Ex: Israel, Pix, Covid)\n",
    "        # O Spacy small adora classificar coisas que não conhece como PROPN\n",
    "        elif token.pos_ == \"PROPN\":\n",
    "            features[\"PROPN\"].append(palavra)\n",
    "\n",
    "            \n",
    "    # Retorna listas únicas\n",
    "    return {k: list(set(v)) for k, v in features.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando Embeddings e Extraindo Entidades... (Isso pode demorar um pouco)\n",
      "Geração de embeddings finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Carregar o modelo gerador de embeddings\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Caminho para salvar o dataframe de notícias\n",
    "data_df_path = \"data/data_df.pkl\"\n",
    "\n",
    "# Selecione diferentes pré-processamentos\n",
    "# Exemplo:\n",
    "\n",
    "preprocessor = Preprocessors()\n",
    "preprocessing_steps = [\n",
    "    preprocessor.remove_urls,\n",
    "    preprocessor.remove_stopwords,\n",
    "    preprocessor.remove_numbers,\n",
    "    preprocessor.lemma,\n",
    "    preprocessor.accented_to_ascii,\n",
    "    preprocessor.lower_case,\n",
    "    #preprocessor.porter_stemmer\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "RECREATE_DF = True\n",
    "\n",
    "# Cria o data frame se ele já existir ou se a variável RECREATE_INDEX for verdadeira\n",
    "# Ou (exclusivo) carrega o dataframe salvo\n",
    "if not os.path.exists(data_df_path) or RECREATE_DF:    \n",
    "    df = pd.read_csv(output_file, sep=\";\")[[\"Título\", \"Texto\", \"Data de Publicação\"]]\n",
    "    df[\"Data de Publicação\"] = df[\"Data de Publicação\"].apply(lambda str_date: datetime.strptime(str_date.split(\" - \")[0], \"%d.%m.%Y\"))\n",
    "    df.sort_values(\"Data de Publicação\", inplace=True, ascending=False)\n",
    "\n",
    "    df = df.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    df[\"Embeddings\"] = [None] * len(df)\n",
    "    df[\"doc_id\"] = df.reset_index(drop=True).index\n",
    "    df[\"entidades_per\"] = [[] for _ in range(len(df))]\n",
    "    df[\"entidades_org\"] = [[] for _ in range(len(df))]\n",
    "    df[\"entidades_loc\"] = [[] for _ in range(len(df))]\n",
    "    df[\"entidades_noun\"] = [[] for _ in range(len(df))]\n",
    "    df[\"entidades_propn\"] = [[] for _ in range(len(df))]\n",
    "    \n",
    "    print(\"Gerando Embeddings e Extraindo Entidades... (Isso pode demorar um pouco)\")\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        texto_completo = row[\"Texto\"].strip() + \"\\n\" + row[\"Título\"].strip()\n",
    "\n",
    "        ents = extrair_features(texto_completo)\n",
    "        df.at[i, \"entidades_per\"] = ents[\"PER\"]\n",
    "        df.at[i, \"entidades_org\"] = ents[\"ORG\"]\n",
    "        df.at[i, \"entidades_loc\"] = ents[\"LOC\"]\n",
    "        df.at[i, \"entidades_noun\"] = ents[\"NOUNS\"]\n",
    "        df.at[i, \"entidades_propn\"] = ents[\"PROPN\"]\n",
    "        \n",
    "        df.at[i, \"Texto completo\"] = texto_completo\n",
    "        texto_processado = texto_completo\n",
    "        for preprocessing_step in preprocessing_steps:\n",
    "            texto_processado = preprocessing_step(texto_processado)\n",
    "        \n",
    "        df.at[i, \"Texto processado\"] = texto_processado\n",
    "        embeddings = model.encode(texto_completo).tolist()\n",
    "        df.at[i, \"Embeddings\"] = embeddings\n",
    "        \n",
    "    print(\"Geração de embeddings finalizada.\")\n",
    "    \n",
    "    with open(data_df_path, \"wb\") as f:\n",
    "        df.to_pickle(f)\n",
    "else:\n",
    "    with open(data_df_path, \"rb\") as f:\n",
    "        df = pd.read_pickle(f)\n",
    "    print(\"Dataframe carregado de arquivo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 3: Indexar dados no ElasticSearch (Lembrem-se de reindexar os dados se os pré-processamentos mudarem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    hosts = [{'host': \"localhost\", 'port': 9200, \"scheme\": \"https\"}],\n",
    "    basic_auth=(\"elastic\",\"elastic\"),\n",
    "    verify_certs = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice 'verificacoes_lupa' criado.\n",
      "Índice preenchido.\n",
      "Indexação finalizada.\n"
     ]
    }
   ],
   "source": [
    "RECREATE_INDEX = True\n",
    "\n",
    "index_name = \"verificacoes_lupa\"\n",
    "\n",
    "# Se a flag for True e se o índice existir, ele é deletado\n",
    "if RECREATE_INDEX and es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "    print(f\"Índice '{index_name}' deletado.\")\n",
    "\n",
    "# Cria o índice e popula com os dados\n",
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(index=index_name, mappings={\n",
    "        \"properties\": {\n",
    "            \"doc_id\": {\"type\": \"integer\"},\n",
    "            \"full_text\": {\"type\": \"text\"},\n",
    "            \"processed_text\": {\"type\": \"text\"},\n",
    "            \"embeddings\": {\"type\": \"dense_vector\", \"dims\": 384},\n",
    "            \"entidades_per\": {\"type\": \"keyword\"},\n",
    "            \"entidades_org\": {\"type\": \"keyword\"},\n",
    "            \"entidades_loc\": {\"type\": \"keyword\"},\n",
    "            \"entidades_noun\": {\"type\": \"keyword\"},\n",
    "            \"entidades_propn\": {\"type\": \"keyword\"},\n",
    "        }\n",
    "    })\n",
    "    print(f\"Índice '{index_name}' criado.\")\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        es.index(index=index_name, id=row[\"doc_id\"], body={\n",
    "            \"doc_id\": row[\"doc_id\"],\n",
    "            \"full_text\": row[\"Texto completo\"],\n",
    "            \"processed_text\": row[\"Texto processado\"],\n",
    "            \"embeddings\": row[\"Embeddings\"],\n",
    "            \"entidades_per\": row[\"entidades_per\"],\n",
    "            \"entidades_org\": row[\"entidades_org\"],\n",
    "            \"entidades_loc\": row[\"entidades_loc\"],\n",
    "            \"entidades_noun\": row[\"entidades_noun\"],\n",
    "            \"entidades_propn\": row[\"entidades_propn\"],\n",
    "        })\n",
    "    print(\"Índice preenchido.\")\n",
    "\n",
    "print(\"Indexação finalizada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estas serão as queries QF1 e QF2\n",
    "with open(\"data/queries_fixadas.txt\", \"r\") as f:\n",
    "    queries_fixadas = [line.strip() for line in f.readlines()]\n",
    "    assert len(queries_fixadas) == 2\n",
    "    QF1 = queries_fixadas[0]\n",
    "    QF2 = queries_fixadas[1]\n",
    "    \n",
    "# Preencha aqui as queries do grupo\n",
    "QP1 = \"vacina causa autismo\"\n",
    "QP2 = \"massacre em Israel\"\n",
    "\n",
    "queries = OrderedDict()\n",
    "queries[\"QF1\"] = QF1\n",
    "queries[\"QF2\"] = QF2\n",
    "queries[\"QP1\"] = QP1\n",
    "queries[\"QP2\"] = QP2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Busca Léxica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação de busca esparsa (léxica) com BM25\n",
    "def lexical_search(queries: dict[str, str]):\n",
    "    lexical_results = {}\n",
    "    for query_id, query in queries.items():\n",
    "        \n",
    "        # Pré-processa os dados\n",
    "        for preprocessing_step in preprocessing_steps:\n",
    "            query = preprocessing_step(query)\n",
    "        \n",
    "        search_query = {\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"processed_text\": query\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Realiza a busca\n",
    "        response = es.search(index=index_name, body=search_query)\n",
    "        \n",
    "        hits_results = []\n",
    "        # Recupera os resultados\n",
    "        for hit in response[\"hits\"][\"hits\"]:\n",
    "            hits_results.append((hit[\"_source\"][\"doc_id\"], hit[\"_score\"]))\n",
    "        lexical_results[query_id] = hits_results\n",
    "        \n",
    "    return lexical_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Busca Semântica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza busca semântica (densa) com KNN exato\n",
    "def semantic_search(queries: dict[str, str]):\n",
    "    semantic_results = {}\n",
    "    \n",
    "    for query_id, query in queries.items():\n",
    "        # Aplica todos os pré-processamentos aos dados\n",
    "        for preprocessing_step in preprocessing_steps:\n",
    "            query = preprocessing_step(query)\n",
    "            \n",
    "        query_vector = model.encode(query).tolist()\n",
    "        \n",
    "        \n",
    "        search_query = {\n",
    "            \"query\": {\n",
    "                \"script_score\": {\n",
    "                    \"query\": {\"match_all\": {}},\n",
    "                    \"script\": {\n",
    "                        \"source\": \"cosineSimilarity(params.query_vector, 'embeddings') + 1.0\",\n",
    "                        \"params\": {\"query_vector\": query_vector}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Realiza a busca\n",
    "        response = es.search(index=index_name, body=search_query)\n",
    "        \n",
    "        hits_results = []\n",
    "        # Recupera top 10 resultados\n",
    "        for hit in response[\"hits\"][\"hits\"]:\n",
    "            hits_results.append((hit[\"_source\"][\"doc_id\"], hit[\"_score\"]))\n",
    "            \n",
    "        semantic_results[query_id] = hits_results\n",
    "\n",
    "    return semantic_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Busca Híbrida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca híbrida ou RRF. Implemente sua solução aqui. Você pode realizar as duas buscas anteriores (léxica e semântica) como base para formar a busca híbrida.\n",
    "\n",
    "def hybrid_search_v1(queries: dict[str, str]):\n",
    "    lexical_results = lexical_search(queries)\n",
    "    semantic_results = semantic_search(queries)\n",
    "\n",
    "    print(lexical_results)\n",
    "    print(semantic_results)\n",
    "\n",
    "    alpha = 0.8\n",
    "    beta = 0.2\n",
    "    k_rrf = 60\n",
    "\n",
    "    relevance_documents = {}\n",
    "\n",
    "    for query in queries.keys():\n",
    "\n",
    "        lexical_rank = {}\n",
    "        semantic_rank = {}\n",
    "\n",
    "        for i, (doc_id, _) in enumerate(lexical_results[query]):\n",
    "            lexical_rank[doc_id] = i + 1\n",
    "\n",
    "        for i, (doc_id, _) in enumerate(semantic_results[query]):\n",
    "            semantic_rank[doc_id] = i + 1\n",
    "\n",
    "        all_docs = set(lexical_rank).union(set(semantic_rank))\n",
    "\n",
    "        relevance_documents[query] = []\n",
    "\n",
    "        for doc_id in all_docs:\n",
    "            rrf_lex = 0\n",
    "            rrf_sem = 0\n",
    "\n",
    "            if doc_id in lexical_rank:\n",
    "                rrf_lex = 1 / (k_rrf + lexical_rank[doc_id])\n",
    "\n",
    "            if doc_id in semantic_rank:\n",
    "                rrf_sem = 1 / (k_rrf + semantic_rank[doc_id])\n",
    "\n",
    "            score = rrf_lex +  rrf_sem\n",
    "            relevance_documents[query].append((doc_id, score))\n",
    "\n",
    "        relevance_documents[query].sort(key=lambda x: x[1], reverse=True)\n",
    "        relevance_documents[query] = relevance_documents[query][:10]\n",
    "\n",
    "    for doc_id, score in relevance_documents[\"QF1\"]:\n",
    "        print(f\"DOC_ID={doc_id} SCORE={score}\")\n",
    "   \n",
    "    return relevance_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca híbrida ou RRF. Implemente sua solução aqui. Você pode realizar as duas buscas anteriores (léxica e semântica) como base para formar a busca híbrida.\n",
    "def hybrid_search_v2(queries: dict[str, str]):\n",
    "    top_k = 10\n",
    "    lexical_results  = lexical_search(queries)\n",
    "    semantic_results = semantic_search(queries)\n",
    "\n",
    "    # print(lexical_results)\n",
    "    # print(semantic_results)\n",
    "\n",
    "    alpha = 0.55\n",
    "    beta  = 0.35\n",
    "    k_rrf = 60\n",
    "    w_rrf = 0.25 \n",
    "\n",
    "    relevance_documents = {}\n",
    "\n",
    "    for qid in queries.keys():\n",
    "        lex_list = lexical_results.get(qid, [])\n",
    "        sem_list = semantic_results.get(qid, [])\n",
    "\n",
    "        lexical_rank   = {doc_id: rank + 1 for rank, (doc_id, _) in enumerate(lex_list)}\n",
    "        semantic_rank  = {doc_id: rank + 1 for rank, (doc_id, _) in enumerate(sem_list)}\n",
    "        lexical_score  = {doc_id: score for (doc_id, score) in lex_list}\n",
    "        semantic_score = {doc_id: score for (doc_id, score) in sem_list}\n",
    "\n",
    "        # Normalizando os scores [0,1]\n",
    "        def normalize_score_map(score_map):\n",
    "            if not score_map: return {}\n",
    "            values = list(score_map.values())\n",
    "            mn, mx = min(values), max(values)\n",
    "            if mx == mn:\n",
    "                return {doc: 1.0 for doc in score_map}\n",
    "            return {doc: (s - mn) / (mx - mn) for doc, s in score_map.items()}\n",
    "\n",
    "        norm_lex = normalize_score_map(lexical_score)\n",
    "        norm_sem = normalize_score_map(semantic_score)\n",
    "\n",
    "        candidate_docs = set(lexical_rank) | set(semantic_rank)\n",
    "\n",
    "        scored = []\n",
    "        for doc_id in candidate_docs:\n",
    "            # RRF baseado nos ranks\n",
    "            rrf_lex = 1.0 / (k_rrf + lexical_rank[doc_id]) if doc_id in lexical_rank else 0.0\n",
    "            rrf_sem = 1.0 / (k_rrf + semantic_rank[doc_id]) if doc_id in semantic_rank else 0.0\n",
    "            rrf_sum = rrf_lex + rrf_sem\n",
    "\n",
    "            nlex = norm_lex.get(doc_id, 0.0)\n",
    "            nsem = norm_sem.get(doc_id, 0.0)\n",
    "\n",
    "            # Juntanto os scores --> scores normalizados + RRF\n",
    "            combined = (alpha * nlex) + (beta * nsem) + (w_rrf * rrf_sum)\n",
    "\n",
    "            scored.append((doc_id, combined))\n",
    "\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "        # relevance_documents[qid] = scored[:top_k]\n",
    "        relevance_documents[qid] = scored[:10]\n",
    "\n",
    "\n",
    "    for doc_id, score in relevance_documents[\"QF1\"]:\n",
    "        print(f\"DOC_ID={doc_id} SCORE={score}\")\n",
    "   \n",
    "    return relevance_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Busca Criativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemente sua própria estratégia de busca, podendo ela ser esparsa, densa ou híbrida. Implemente algo como \"more_like_this\", \"BM35\", \"fuzzy\" etc.\n",
    "def creative_search_v1(queries: dict[str, str]):\n",
    "    # 1. Extrai entidades da PERGUNTA do usuário\n",
    "    resultados_queries = {}\n",
    "\n",
    "    for query_id, query in queries.items():\n",
    "        ents_query = extrair_features(query)\n",
    "        print(f\"Entidades da {query_id}, {query}\", ents_query)\n",
    "        q_per = ents_query[\"PER\"]\n",
    "        q_org = ents_query[\"ORG\"]\n",
    "        q_loc = ents_query[\"LOC\"]\n",
    "        q_noun = ents_query[\"NOUNS\"]\n",
    "        q_propn = ents_query[\"PROPN\"]\n",
    "        for processor in preprocessing_steps:\n",
    "            query = processor(query)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Gera vetor da query\n",
    "        query_vector = model.encode(query).tolist()\n",
    "        \n",
    "        # 2. Monta a Query com Boost de Entidade\n",
    "        body = {\n",
    "            \"size\": 10,\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    # A. O \"Must\" garante que o assunto seja relevante (Híbrida Simples)\n",
    "                    \"must\": [\n",
    "                        {\n",
    "                            \"bool\": {\n",
    "                                \"should\": [\n",
    "                                    {\"match\": {\"processed_text\": query}}, # BM25\n",
    "                                    {\"knn\": {\"field\": \"embeddings\", \"query_vector\": query_vector, \"k\": 10, \"boost\": 0.5}} # Vetorial\n",
    "                                ]\n",
    "                            }\n",
    "                        }\n",
    "                    ],\n",
    "                    \n",
    "                    # B. O \"Should\" é o bônus da Busca Criativa\n",
    "                    # Se bater a entidade, a nota sobe.\n",
    "                    \"should\": [\n",
    "                        {\"terms\": {\"entidades_per\": q_per, \"boost\": 5.0}},\n",
    "                        {\"terms\": {\"entidades_org\": q_org, \"boost\": 5.0}},\n",
    "                        {\"terms\": {\"entidades_loc\": q_loc, \"boost\": 4.0}},\n",
    "\n",
    "                        {\"terms\": {\"entidades_noun\": q_noun, \"boost\": 4.0}},\n",
    "                        {\"terms\": {\"entidades_propn\": q_propn, \"boost\": 3.5}},\n",
    "\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Executa a busca\n",
    "        res = es.search(index=index_name, body=body)\n",
    "        \n",
    "        # Formata retorno apenas com doc_id (conforme pedido na competição)\n",
    "        resultados = []\n",
    "        for hit in res['hits']['hits']:\n",
    "            resultados.append((hit['_source']['doc_id'], hit[\"_score\"]))\n",
    "        \n",
    "        resultados_queries[query_id] = resultados\n",
    "        \n",
    "    return resultados_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemente sua própria estratégia de busca, podendo ela ser esparsa, densa ou híbrida. Implemente algo como \"more_like_this\", \"BM35\", \"fuzzy\" etc.\n",
    "def creative_search_v2(queries: dict[str, str]):\n",
    "    size = 10\n",
    "\n",
    "    resultados_queries = {}\n",
    "\n",
    "    for query_id, query in queries.items():\n",
    "        processed_query = query\n",
    "        for processor in preprocessing_steps:\n",
    "            processed_query = processor(processed_query)\n",
    "\n",
    "        ents = extrair_features(query)\n",
    "        q_per   = ents.get(\"PER\", []) or []\n",
    "        q_org   = ents.get(\"ORG\", []) or []\n",
    "        q_loc   = ents.get(\"LOC\", []) or []\n",
    "        q_noun  = ents.get(\"NOUNS\", []) or []\n",
    "        q_propn = ents.get(\"PROPN\", []) or []\n",
    "\n",
    "        should_clauses = []\n",
    "\n",
    "        # 1) \"more_like_this\" em \"full_text\" p/ ajudar a encontrar documentos semanticamente semelhantes\n",
    "        mlt_clause = {\n",
    "            \"more_like_this\": {\n",
    "                \"fields\": [\"full_text\"],\n",
    "                \"like\": [query],\n",
    "                \"min_term_freq\": 1,\n",
    "                \"min_doc_freq\": 1,\n",
    "                \"max_query_terms\": 25\n",
    "            }\n",
    "        }\n",
    "        should_clauses.append({\"bool\": {\"must\": mlt_clause}})\n",
    "\n",
    "        # 2) \"multi_match\" em \"processed_text\" com fuzziness e cross_fields\n",
    "        multi_match_clause = {\n",
    "            \"multi_match\": {\n",
    "                \"query\": processed_query,\n",
    "                \"fields\": [\"processed_text\"],\n",
    "                \"type\": \"best_fields\",\n",
    "                \"operator\": \"and\",     # prefira documentos que correspondam a todos os termos da pesquisa\n",
    "                \"fuzziness\": \"AUTO\",\n",
    "                \"prefix_length\": 2\n",
    "            }\n",
    "        }\n",
    "        should_clauses.append(multi_match_clause)\n",
    "\n",
    "        # 3) Boosted entity com matches exatas: usando termo/match nos entity fields\n",
    "        #    Documentos que contenham named entities da query serao favorecidos\n",
    "        entity_boosts = [\n",
    "            (\"entidades_per\", q_per, 6.0),\n",
    "            (\"entidades_org\", q_org, 5.0),\n",
    "            (\"entidades_loc\", q_loc, 4.0),\n",
    "            (\"entidades_propn\", q_propn, 3.5),\n",
    "            (\"entidades_noun\", q_noun, 2.5),\n",
    "        ]\n",
    "\n",
    "        for field, values, boost in entity_boosts:\n",
    "            if values:\n",
    "                for v in values:\n",
    "                    v_proc = v.strip()\n",
    "                    if not v_proc: continue\n",
    "                    should_clauses.append({\n",
    "                        \"match\": {\n",
    "                            field: {\n",
    "                                \"query\": v_proc,\n",
    "                                \"boost\": float(boost),\n",
    "                                \"operator\": \"and\"\n",
    "                            }\n",
    "                        }\n",
    "                    })\n",
    "\n",
    "        # 4) Uma correspondência de frases simplificada em \"processed_text\" para priorizar match de frases (maior precisão)\n",
    "        phrase_clause = {\n",
    "            \"match_phrase\": {\n",
    "                \"processed_text\": {\n",
    "                    \"query\": processed_query,\n",
    "                    \"slop\": 2,\n",
    "                    \"boost\": 1.8\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        should_clauses.append(phrase_clause)\n",
    "\n",
    "        # Cria a query final\n",
    "        body = {\n",
    "            \"size\": size,\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                        # Pelo menos uma das classes vai dar match, mas prefira usar multiplas matches\n",
    "                        \"should\": should_clauses,\n",
    "                        \"minimum_should_match\": 1,\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "        # Executa a busca\n",
    "        res = es.search(index=index_name, body=body)\n",
    "\n",
    "        resultados = []\n",
    "        for hit in res.get(\"hits\", {}).get(\"hits\", []):\n",
    "            doc_id = hit.get(\"_source\", {}).get(\"doc_id\", hit.get(\"_id\"))\n",
    "            score  = hit.get(\"_score\", 0.0)\n",
    "            resultados.append((doc_id, score))\n",
    "\n",
    "        resultados_queries[query_id] = resultados\n",
    "\n",
    "    return resultados_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execução das buscas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'QF1': [(1429, 16.823084), (299, 16.192177), (140, 16.191858), (1689, 15.490684), (2328, 14.387486), (1398, 14.052032), (1735, 13.307976), (410, 12.483434), (1887, 12.423523), (2021, 12.145069)], 'QF2': [(1594, 12.996189), (1237, 12.85702), (1621, 12.774794), (316, 12.742256), (2208, 12.732671), (15, 12.723009), (1067, 12.273449), (510, 12.072852), (122, 11.790132), (1793, 11.518058)], 'QP1': [(2314, 11.017654), (303, 10.456556), (960, 9.499834), (1394, 9.259333), (2519, 8.568039), (2436, 8.433064), (1892, 8.372154), (1552, 8.238545), (1296, 7.72898), (294, 7.6265297)], 'QP2': [(814, 15.349197), (1388, 13.525953), (180, 12.836807), (1992, 11.048485), (1133, 8.7437725), (1578, 8.4807625), (2484, 7.2963786), (1235, 7.2920065), (2550, 7.282001), (2492, 7.243937)]}\n",
      "{'QF1': [(1398, 1.6246119), (2188, 1.5283167), (1429, 1.5064515), (2328, 1.505476), (2555, 1.4939659), (333, 1.4828167), (177, 1.4721811), (410, 1.47214), (2410, 1.4565318), (1125, 1.4546205)], 'QF2': [(1237, 1.8076564), (1722, 1.7621573), (1989, 1.7520218), (1621, 1.7430637), (510, 1.7271051), (1752, 1.7106292), (1132, 1.7047324), (2073, 1.7021568), (861, 1.701055), (1594, 1.7007091)], 'QP1': [(1237, 1.5580077), (359, 1.5565056), (2519, 1.5390477), (186, 1.5387577), (137, 1.5309662), (1752, 1.519625), (121, 1.5192796), (1594, 1.51811), (2498, 1.5166239), (1027, 1.514234)], 'QP2': [(814, 1.7237178), (1693, 1.6455503), (1023, 1.6314374), (2123, 1.6291356), (180, 1.617863), (1388, 1.6123211), (1378, 1.6045778), (304, 1.5905678), (1790, 1.5806785), (2566, 1.5670909)]}\n",
      "DOC_ID=1429 SCORE=0.032266458495966696\n",
      "DOC_ID=1398 SCORE=0.031544957774465976\n",
      "DOC_ID=2328 SCORE=0.031009615384615385\n",
      "DOC_ID=410 SCORE=0.029411764705882353\n",
      "DOC_ID=2188 SCORE=0.016129032258064516\n",
      "DOC_ID=299 SCORE=0.016129032258064516\n",
      "DOC_ID=140 SCORE=0.015873015873015872\n",
      "DOC_ID=1689 SCORE=0.015625\n",
      "DOC_ID=2555 SCORE=0.015384615384615385\n",
      "DOC_ID=333 SCORE=0.015151515151515152\n",
      "DOC_ID=1429 SCORE=0.6647828955652628\n",
      "DOC_ID=1398 SCORE=0.5820902340866436\n",
      "DOC_ID=299 SCORE=0.47985574302555195\n",
      "DOC_ID=140 SCORE=0.47975423370538595\n",
      "DOC_ID=1689 SCORE=0.3972543367419193\n",
      "DOC_ID=2328 SCORE=0.3761039364731097\n",
      "DOC_ID=2188 SCORE=0.15576740466604982\n",
      "DOC_ID=1735 SCORE=0.14045575524036294\n",
      "DOC_ID=2555 SCORE=0.08485548725949116\n",
      "DOC_ID=410 SCORE=0.08320631565342534\n",
      "Entidades da QF1, Haverá cobrança na transferência bancária digital a partir de agora {'PER': [], 'ORG': [], 'LOC': [], 'NOUNS': ['partir', 'transferência', 'cobrança'], 'PROPN': []}\n",
      "Entidades da QF2, As vacinas da covid19 causaram mortes {'PER': [], 'ORG': [], 'LOC': [], 'NOUNS': ['covid19', 'vacinas', 'mortes'], 'PROPN': []}\n",
      "Entidades da QP1, vacina causa autismo {'PER': [], 'ORG': [], 'LOC': [], 'NOUNS': ['causa', 'vacina'], 'PROPN': []}\n",
      "Entidades da QP2, massacre em Israel {'PER': [], 'ORG': [], 'LOC': ['Israel'], 'NOUNS': ['massacre'], 'PROPN': ['israel']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lexical': {'QF1': [(1429, 16.823084),\n",
       "   (299, 16.192177),\n",
       "   (140, 16.191858),\n",
       "   (1689, 15.490684),\n",
       "   (2328, 14.387486),\n",
       "   (1398, 14.052032),\n",
       "   (1735, 13.307976),\n",
       "   (410, 12.483434),\n",
       "   (1887, 12.423523),\n",
       "   (2021, 12.145069)],\n",
       "  'QF2': [(1594, 12.996189),\n",
       "   (1237, 12.85702),\n",
       "   (1621, 12.774794),\n",
       "   (316, 12.742256),\n",
       "   (2208, 12.732671),\n",
       "   (15, 12.723009),\n",
       "   (1067, 12.273449),\n",
       "   (510, 12.072852),\n",
       "   (122, 11.790132),\n",
       "   (1793, 11.518058)],\n",
       "  'QP1': [(2314, 11.017654),\n",
       "   (303, 10.456556),\n",
       "   (960, 9.499834),\n",
       "   (1394, 9.259333),\n",
       "   (2519, 8.568039),\n",
       "   (2436, 8.433064),\n",
       "   (1892, 8.372154),\n",
       "   (1552, 8.238545),\n",
       "   (1296, 7.72898),\n",
       "   (294, 7.6265297)],\n",
       "  'QP2': [(814, 15.349197),\n",
       "   (1388, 13.525953),\n",
       "   (180, 12.836807),\n",
       "   (1992, 11.048485),\n",
       "   (1133, 8.7437725),\n",
       "   (1578, 8.4807625),\n",
       "   (2484, 7.2963786),\n",
       "   (1235, 7.2920065),\n",
       "   (2550, 7.282001),\n",
       "   (2492, 7.243937)]},\n",
       " 'semantic': {'QF1': [(1398, 1.6246119),\n",
       "   (2188, 1.5283167),\n",
       "   (1429, 1.5064515),\n",
       "   (2328, 1.505476),\n",
       "   (2555, 1.4939659),\n",
       "   (333, 1.4828167),\n",
       "   (177, 1.4721811),\n",
       "   (410, 1.47214),\n",
       "   (2410, 1.4565318),\n",
       "   (1125, 1.4546205)],\n",
       "  'QF2': [(1237, 1.8076564),\n",
       "   (1722, 1.7621573),\n",
       "   (1989, 1.7520218),\n",
       "   (1621, 1.7430637),\n",
       "   (510, 1.7271051),\n",
       "   (1752, 1.7106292),\n",
       "   (1132, 1.7047324),\n",
       "   (2073, 1.7021568),\n",
       "   (861, 1.701055),\n",
       "   (1594, 1.7007091)],\n",
       "  'QP1': [(1237, 1.5580077),\n",
       "   (359, 1.5565056),\n",
       "   (2519, 1.5390477),\n",
       "   (186, 1.5387577),\n",
       "   (137, 1.5309662),\n",
       "   (1752, 1.519625),\n",
       "   (121, 1.5192796),\n",
       "   (1594, 1.51811),\n",
       "   (2498, 1.5166239),\n",
       "   (1027, 1.514234)],\n",
       "  'QP2': [(814, 1.7237178),\n",
       "   (1693, 1.6455503),\n",
       "   (1023, 1.6314374),\n",
       "   (2123, 1.6291356),\n",
       "   (180, 1.617863),\n",
       "   (1388, 1.6123211),\n",
       "   (1378, 1.6045778),\n",
       "   (304, 1.5905678),\n",
       "   (1790, 1.5806785),\n",
       "   (2566, 1.5670909)]},\n",
       " 'hybrid_v1': {'QF1': [(1429, 0.032266458495966696),\n",
       "   (1398, 0.031544957774465976),\n",
       "   (2328, 0.031009615384615385),\n",
       "   (410, 0.029411764705882353),\n",
       "   (2188, 0.016129032258064516),\n",
       "   (299, 0.016129032258064516),\n",
       "   (140, 0.015873015873015872),\n",
       "   (1689, 0.015625),\n",
       "   (2555, 0.015384615384615385),\n",
       "   (333, 0.015151515151515152)],\n",
       "  'QF2': [(1237, 0.03252247488101534),\n",
       "   (1621, 0.03149801587301587),\n",
       "   (1594, 0.030679156908665108),\n",
       "   (510, 0.030090497737556562),\n",
       "   (1722, 0.016129032258064516),\n",
       "   (1989, 0.015873015873015872),\n",
       "   (316, 0.015625),\n",
       "   (2208, 0.015384615384615385),\n",
       "   (15, 0.015151515151515152),\n",
       "   (1752, 0.015151515151515152)],\n",
       "  'QP1': [(2519, 0.03125763125763126),\n",
       "   (2314, 0.01639344262295082),\n",
       "   (1237, 0.01639344262295082),\n",
       "   (359, 0.016129032258064516),\n",
       "   (303, 0.016129032258064516),\n",
       "   (960, 0.015873015873015872),\n",
       "   (186, 0.015625),\n",
       "   (1394, 0.015625),\n",
       "   (137, 0.015384615384615385),\n",
       "   (2436, 0.015151515151515152)],\n",
       "  'QP2': [(814, 0.03278688524590164),\n",
       "   (1388, 0.03128054740957967),\n",
       "   (180, 0.03125763125763126),\n",
       "   (1693, 0.016129032258064516),\n",
       "   (1023, 0.015873015873015872),\n",
       "   (1992, 0.015625),\n",
       "   (2123, 0.015625),\n",
       "   (1133, 0.015384615384615385),\n",
       "   (1578, 0.015151515151515152),\n",
       "   (1378, 0.014925373134328358)]},\n",
       " 'hybrid_v2': {'QF1': [(1429, 0.6647828955652628),\n",
       "   (1398, 0.5820902340866436),\n",
       "   (299, 0.47985574302555195),\n",
       "   (140, 0.47975423370538595),\n",
       "   (1689, 0.3972543367419193),\n",
       "   (2328, 0.3761039364731097),\n",
       "   (2188, 0.15576740466604982),\n",
       "   (1735, 0.14045575524036294),\n",
       "   (2555, 0.08485548725949116),\n",
       "   (410, 0.08320631565342534)],\n",
       "  'QF2': [(1237, 0.856347014966595),\n",
       "   (1621, 0.614106656920208),\n",
       "   (1594, 0.5576697892271664),\n",
       "   (316, 0.4594199358641081),\n",
       "   (2208, 0.45579334255946835),\n",
       "   (15, 0.45213991930390884),\n",
       "   (510, 0.3003413657316511),\n",
       "   (1067, 0.2848059232768303),\n",
       "   (1722, 0.20513008849127745),\n",
       "   (1989, 0.17189624280013677)],\n",
       "  'QP1': [(2314, 0.5540983606557377),\n",
       "   (303, 0.46302884070231576),\n",
       "   (2519, 0.35891813308361875),\n",
       "   (1237, 0.3540983606557377),\n",
       "   (359, 0.3420219642122701),\n",
       "   (960, 0.30779591519553484),\n",
       "   (1394, 0.2687275114913585),\n",
       "   (186, 0.19998962883249372),\n",
       "   (137, 0.13763128053181095),\n",
       "   (2436, 0.13459814280562654)],\n",
       "  'QP2': [(814, 0.9081967213114754),\n",
       "   (1388, 0.5351718066129629),\n",
       "   (180, 0.5007865646335209),\n",
       "   (1992, 0.2620721077269575),\n",
       "   (1693, 0.17935839935953007),\n",
       "   (1023, 0.14775757112897175),\n",
       "   (2123, 0.14255194240660435),\n",
       "   (1133, 0.10562074528430637),\n",
       "   (1578, 0.08771535612975313),\n",
       "   (1378, 0.08749993603489105)]},\n",
       " 'creative_v1': {'QF1': [(1429, 21.199839),\n",
       "   (299, 20.192177),\n",
       "   (1689, 19.490685),\n",
       "   (2328, 18.764454),\n",
       "   (1735, 17.307976),\n",
       "   (1887, 16.423523),\n",
       "   (140, 16.191858),\n",
       "   (1834, 15.6868105),\n",
       "   (2477, 15.374478),\n",
       "   (2205, 14.695587)],\n",
       "  'QF2': [(1594, 17.421227),\n",
       "   (1237, 17.3088),\n",
       "   (1621, 17.210634),\n",
       "   (316, 16.742256),\n",
       "   (2208, 16.73267),\n",
       "   (15, 16.72301),\n",
       "   (510, 16.504848),\n",
       "   (1067, 16.273449),\n",
       "   (861, 15.857356),\n",
       "   (122, 15.790132)],\n",
       "  'QP1': [(2314, 15.017654),\n",
       "   (303, 14.456556),\n",
       "   (960, 13.499834),\n",
       "   (1394, 13.259333),\n",
       "   (2519, 12.95436),\n",
       "   (2436, 12.433064),\n",
       "   (1892, 12.372154),\n",
       "   (1552, 12.238545),\n",
       "   (121, 11.743575),\n",
       "   (1296, 11.72898)],\n",
       "  'QP2': [(814, 27.281872),\n",
       "   (1992, 22.548485),\n",
       "   (1388, 21.430708),\n",
       "   (180, 20.742607),\n",
       "   (2492, 15.136376),\n",
       "   (304, 14.903063),\n",
       "   (2123, 14.846941),\n",
       "   (1790, 14.835602),\n",
       "   (1023, 14.803107),\n",
       "   (2484, 14.796379)]},\n",
       " 'creative_v2': {'QF1': [(299, 49.295006),\n",
       "   (1887, 44.67344),\n",
       "   (1561, 42.527287),\n",
       "   (1295, 41.806232),\n",
       "   (773, 40.941246),\n",
       "   (2365, 37.90988),\n",
       "   (1429, 37.33433),\n",
       "   (1342, 32.685303),\n",
       "   (1865, 32.224113),\n",
       "   (1999, 31.991785)],\n",
       "  'QF2': [(1621, 59.45197),\n",
       "   (1237, 57.21261),\n",
       "   (861, 53.0961),\n",
       "   (1594, 52.2705),\n",
       "   (1349, 50.78158),\n",
       "   (510, 49.370655),\n",
       "   (1258, 48.456974),\n",
       "   (316, 46.862865),\n",
       "   (2208, 46.75454),\n",
       "   (15, 46.453953)],\n",
       "  'QP1': [(960, 46.01397),\n",
       "   (303, 36.968796),\n",
       "   (595, 35.663166),\n",
       "   (1892, 30.918139),\n",
       "   (2314, 29.38421),\n",
       "   (1394, 26.941202),\n",
       "   (2235, 26.5892),\n",
       "   (2519, 26.36695),\n",
       "   (1185, 26.227802),\n",
       "   (1067, 26.144047)],\n",
       "  'QP2': [(814, 111.52178),\n",
       "   (1992, 88.82996),\n",
       "   (180, 73.64378),\n",
       "   (1388, 64.49675),\n",
       "   (2550, 51.925663),\n",
       "   (2484, 51.916473),\n",
       "   (1235, 51.91301),\n",
       "   (2492, 51.85422),\n",
       "   (1779, 51.74986),\n",
       "   (497, 51.712227)]}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_functions = [\n",
    "    (\"lexical\", lexical_search),\n",
    "    (\"semantic\", semantic_search),\n",
    "    (\"hybrid_v1\", hybrid_search_v1),\n",
    "    (\"hybrid_v2\", hybrid_search_v2),\n",
    "    (\"creative_v1\", creative_search_v1),\n",
    "    (\"creative_v2\", creative_search_v2)\n",
    "]\n",
    "\n",
    "def run_all_searches(queries: dict[str, str]):\n",
    "    all_search_results = {}\n",
    "    for search_name, search_function in search_functions:\n",
    "        results = search_function(queries)\n",
    "        all_search_results[search_name] = results\n",
    "    return all_search_results\n",
    "\n",
    "run_all_searches(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analise os resultados da busca e aprimore a busca!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'QF1': [(1429, 16.823084), (299, 16.192177), (140, 16.191858), (1689, 15.490684), (2328, 14.387486), (1398, 14.052032), (1735, 13.307976), (410, 12.483434), (1887, 12.423523), (2021, 12.145069)], 'QF2': [(1594, 12.996189), (1237, 12.85702), (1621, 12.774794), (316, 12.742256), (2208, 12.732671), (15, 12.723009), (1067, 12.273449), (510, 12.072852), (122, 11.790132), (1793, 11.518058)], 'QP1': [(2314, 11.017654), (303, 10.456556), (960, 9.499834), (1394, 9.259333), (2519, 8.568039), (2436, 8.433064), (1892, 8.372154), (1552, 8.238545), (1296, 7.72898), (294, 7.6265297)], 'QP2': [(814, 15.349197), (1388, 13.525953), (180, 12.836807), (1992, 11.048485), (1133, 8.7437725), (1578, 8.4807625), (2484, 7.2963786), (1235, 7.2920065), (2550, 7.282001), (2492, 7.243937)]}\n",
      "{'QF1': [(1398, 1.6246119), (2188, 1.5283167), (1429, 1.5064515), (2328, 1.505476), (2555, 1.4939659), (333, 1.4828167), (177, 1.4721811), (410, 1.47214), (2410, 1.4565318), (1125, 1.4546205)], 'QF2': [(1237, 1.8076564), (1722, 1.7621573), (1989, 1.7520218), (1621, 1.7430637), (510, 1.7271051), (1752, 1.7106292), (1132, 1.7047324), (2073, 1.7021568), (861, 1.701055), (1594, 1.7007091)], 'QP1': [(1237, 1.5580077), (359, 1.5565056), (2519, 1.5390477), (186, 1.5387577), (137, 1.5309662), (1752, 1.519625), (121, 1.5192796), (1594, 1.51811), (2498, 1.5166239), (1027, 1.514234)], 'QP2': [(814, 1.7237178), (1693, 1.6455503), (1023, 1.6314374), (2123, 1.6291356), (180, 1.617863), (1388, 1.6123211), (1378, 1.6045778), (304, 1.5905678), (1790, 1.5806785), (2566, 1.5670909)]}\n",
      "DOC_ID=1429 SCORE=0.032266458495966696\n",
      "DOC_ID=1398 SCORE=0.031544957774465976\n",
      "DOC_ID=2328 SCORE=0.031009615384615385\n",
      "DOC_ID=410 SCORE=0.029411764705882353\n",
      "DOC_ID=2188 SCORE=0.016129032258064516\n",
      "DOC_ID=299 SCORE=0.016129032258064516\n",
      "DOC_ID=140 SCORE=0.015873015873015872\n",
      "DOC_ID=1689 SCORE=0.015625\n",
      "DOC_ID=2555 SCORE=0.015384615384615385\n",
      "DOC_ID=333 SCORE=0.015151515151515152\n",
      "DOC_ID=1429 SCORE=0.6647828955652628\n",
      "DOC_ID=1398 SCORE=0.5820902340866436\n",
      "DOC_ID=299 SCORE=0.47985574302555195\n",
      "DOC_ID=140 SCORE=0.47975423370538595\n",
      "DOC_ID=1689 SCORE=0.3972543367419193\n",
      "DOC_ID=2328 SCORE=0.3761039364731097\n",
      "DOC_ID=2188 SCORE=0.15576740466604982\n",
      "DOC_ID=1735 SCORE=0.14045575524036294\n",
      "DOC_ID=2555 SCORE=0.08485548725949116\n",
      "DOC_ID=410 SCORE=0.08320631565342534\n",
      "Entidades da QF1, Haverá cobrança na transferência bancária digital a partir de agora {'PER': [], 'ORG': [], 'LOC': [], 'NOUNS': ['partir', 'transferência', 'cobrança'], 'PROPN': []}\n",
      "Entidades da QF2, As vacinas da covid19 causaram mortes {'PER': [], 'ORG': [], 'LOC': [], 'NOUNS': ['covid19', 'vacinas', 'mortes'], 'PROPN': []}\n",
      "Entidades da QP1, vacina causa autismo {'PER': [], 'ORG': [], 'LOC': [], 'NOUNS': ['causa', 'vacina'], 'PROPN': []}\n",
      "Entidades da QP2, massacre em Israel {'PER': [], 'ORG': [], 'LOC': ['Israel'], 'NOUNS': ['massacre'], 'PROPN': ['israel']}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexical</th>\n",
       "      <th>semantic</th>\n",
       "      <th>hybrid_v1</th>\n",
       "      <th>hybrid_v2</th>\n",
       "      <th>creative_v1</th>\n",
       "      <th>creative_v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>[(1429, 16.823084), (299, 16.192177), (140, 16...</td>\n",
       "      <td>[(1398, 1.6246119), (2188, 1.5283167), (1429, ...</td>\n",
       "      <td>[(1429, 0.032266458495966696), (1398, 0.031544...</td>\n",
       "      <td>[(1429, 0.6647828955652628), (1398, 0.58209023...</td>\n",
       "      <td>[(1429, 21.199839), (299, 20.192177), (1689, 1...</td>\n",
       "      <td>[(299, 49.295006), (1887, 44.67344), (1561, 42...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>[(1594, 12.996189), (1237, 12.85702), (1621, 1...</td>\n",
       "      <td>[(1237, 1.8076564), (1722, 1.7621573), (1989, ...</td>\n",
       "      <td>[(1237, 0.03252247488101534), (1621, 0.0314980...</td>\n",
       "      <td>[(1237, 0.856347014966595), (1621, 0.614106656...</td>\n",
       "      <td>[(1594, 17.421227), (1237, 17.3088), (1621, 17...</td>\n",
       "      <td>[(1621, 59.45197), (1237, 57.21261), (861, 53....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>[(2314, 11.017654), (303, 10.456556), (960, 9....</td>\n",
       "      <td>[(1237, 1.5580077), (359, 1.5565056), (2519, 1...</td>\n",
       "      <td>[(2519, 0.03125763125763126), (2314, 0.0163934...</td>\n",
       "      <td>[(2314, 0.5540983606557377), (303, 0.463028840...</td>\n",
       "      <td>[(2314, 15.017654), (303, 14.456556), (960, 13...</td>\n",
       "      <td>[(960, 46.01397), (303, 36.968796), (595, 35.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>[(814, 15.349197), (1388, 13.525953), (180, 12...</td>\n",
       "      <td>[(814, 1.7237178), (1693, 1.6455503), (1023, 1...</td>\n",
       "      <td>[(814, 0.03278688524590164), (1388, 0.03128054...</td>\n",
       "      <td>[(814, 0.9081967213114754), (1388, 0.535171806...</td>\n",
       "      <td>[(814, 27.281872), (1992, 22.548485), (1388, 2...</td>\n",
       "      <td>[(814, 111.52178), (1992, 88.82996), (180, 73....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               lexical  \\\n",
       "QF1  [(1429, 16.823084), (299, 16.192177), (140, 16...   \n",
       "QF2  [(1594, 12.996189), (1237, 12.85702), (1621, 1...   \n",
       "QP1  [(2314, 11.017654), (303, 10.456556), (960, 9....   \n",
       "QP2  [(814, 15.349197), (1388, 13.525953), (180, 12...   \n",
       "\n",
       "                                              semantic  \\\n",
       "QF1  [(1398, 1.6246119), (2188, 1.5283167), (1429, ...   \n",
       "QF2  [(1237, 1.8076564), (1722, 1.7621573), (1989, ...   \n",
       "QP1  [(1237, 1.5580077), (359, 1.5565056), (2519, 1...   \n",
       "QP2  [(814, 1.7237178), (1693, 1.6455503), (1023, 1...   \n",
       "\n",
       "                                             hybrid_v1  \\\n",
       "QF1  [(1429, 0.032266458495966696), (1398, 0.031544...   \n",
       "QF2  [(1237, 0.03252247488101534), (1621, 0.0314980...   \n",
       "QP1  [(2519, 0.03125763125763126), (2314, 0.0163934...   \n",
       "QP2  [(814, 0.03278688524590164), (1388, 0.03128054...   \n",
       "\n",
       "                                             hybrid_v2  \\\n",
       "QF1  [(1429, 0.6647828955652628), (1398, 0.58209023...   \n",
       "QF2  [(1237, 0.856347014966595), (1621, 0.614106656...   \n",
       "QP1  [(2314, 0.5540983606557377), (303, 0.463028840...   \n",
       "QP2  [(814, 0.9081967213114754), (1388, 0.535171806...   \n",
       "\n",
       "                                           creative_v1  \\\n",
       "QF1  [(1429, 21.199839), (299, 20.192177), (1689, 1...   \n",
       "QF2  [(1594, 17.421227), (1237, 17.3088), (1621, 17...   \n",
       "QP1  [(2314, 15.017654), (303, 14.456556), (960, 13...   \n",
       "QP2  [(814, 27.281872), (1992, 22.548485), (1388, 2...   \n",
       "\n",
       "                                           creative_v2  \n",
       "QF1  [(299, 49.295006), (1887, 44.67344), (1561, 42...  \n",
       "QF2  [(1621, 59.45197), (1237, 57.21261), (861, 53....  \n",
       "QP1  [(960, 46.01397), (303, 36.968796), (595, 35.6...  \n",
       "QP2  [(814, 111.52178), (1992, 88.82996), (180, 73....  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_search_results = run_all_searches(queries)\n",
    "search_results_df = pd.DataFrame(all_search_results)\n",
    "search_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados das buscas salvos em 'data/search_results.csv'.\n",
      "Documentos de interesse salvos em 'data/documents.csv'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexical</th>\n",
       "      <th>semantic</th>\n",
       "      <th>hybrid_v1</th>\n",
       "      <th>hybrid_v2</th>\n",
       "      <th>creative_v1</th>\n",
       "      <th>creative_v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1429</td>\n",
       "      <td>1398</td>\n",
       "      <td>1429</td>\n",
       "      <td>1429</td>\n",
       "      <td>1429</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>299</td>\n",
       "      <td>2188</td>\n",
       "      <td>1398</td>\n",
       "      <td>1398</td>\n",
       "      <td>299</td>\n",
       "      <td>1887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140</td>\n",
       "      <td>1429</td>\n",
       "      <td>2328</td>\n",
       "      <td>299</td>\n",
       "      <td>1689</td>\n",
       "      <td>1561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1689</td>\n",
       "      <td>2328</td>\n",
       "      <td>410</td>\n",
       "      <td>140</td>\n",
       "      <td>2328</td>\n",
       "      <td>1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2328</td>\n",
       "      <td>2555</td>\n",
       "      <td>2188</td>\n",
       "      <td>1689</td>\n",
       "      <td>1735</td>\n",
       "      <td>773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1398</td>\n",
       "      <td>333</td>\n",
       "      <td>299</td>\n",
       "      <td>2328</td>\n",
       "      <td>1887</td>\n",
       "      <td>2365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1735</td>\n",
       "      <td>177</td>\n",
       "      <td>140</td>\n",
       "      <td>2188</td>\n",
       "      <td>140</td>\n",
       "      <td>1429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>1689</td>\n",
       "      <td>1735</td>\n",
       "      <td>1834</td>\n",
       "      <td>1342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1887</td>\n",
       "      <td>2410</td>\n",
       "      <td>2555</td>\n",
       "      <td>2555</td>\n",
       "      <td>2477</td>\n",
       "      <td>1865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021</td>\n",
       "      <td>1125</td>\n",
       "      <td>333</td>\n",
       "      <td>410</td>\n",
       "      <td>2205</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1594</td>\n",
       "      <td>1237</td>\n",
       "      <td>1237</td>\n",
       "      <td>1237</td>\n",
       "      <td>1594</td>\n",
       "      <td>1621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1237</td>\n",
       "      <td>1722</td>\n",
       "      <td>1621</td>\n",
       "      <td>1621</td>\n",
       "      <td>1237</td>\n",
       "      <td>1237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1621</td>\n",
       "      <td>1989</td>\n",
       "      <td>1594</td>\n",
       "      <td>1594</td>\n",
       "      <td>1621</td>\n",
       "      <td>861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>316</td>\n",
       "      <td>1621</td>\n",
       "      <td>510</td>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "      <td>1594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2208</td>\n",
       "      <td>510</td>\n",
       "      <td>1722</td>\n",
       "      <td>2208</td>\n",
       "      <td>2208</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>1752</td>\n",
       "      <td>1989</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1067</td>\n",
       "      <td>1132</td>\n",
       "      <td>316</td>\n",
       "      <td>510</td>\n",
       "      <td>510</td>\n",
       "      <td>1258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>510</td>\n",
       "      <td>2073</td>\n",
       "      <td>2208</td>\n",
       "      <td>1067</td>\n",
       "      <td>1067</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>122</td>\n",
       "      <td>861</td>\n",
       "      <td>15</td>\n",
       "      <td>1722</td>\n",
       "      <td>861</td>\n",
       "      <td>2208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1793</td>\n",
       "      <td>1594</td>\n",
       "      <td>1752</td>\n",
       "      <td>1989</td>\n",
       "      <td>122</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2314</td>\n",
       "      <td>1237</td>\n",
       "      <td>2519</td>\n",
       "      <td>2314</td>\n",
       "      <td>2314</td>\n",
       "      <td>960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>303</td>\n",
       "      <td>359</td>\n",
       "      <td>2314</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>960</td>\n",
       "      <td>2519</td>\n",
       "      <td>1237</td>\n",
       "      <td>2519</td>\n",
       "      <td>960</td>\n",
       "      <td>595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1394</td>\n",
       "      <td>186</td>\n",
       "      <td>359</td>\n",
       "      <td>1237</td>\n",
       "      <td>1394</td>\n",
       "      <td>1892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2519</td>\n",
       "      <td>137</td>\n",
       "      <td>303</td>\n",
       "      <td>359</td>\n",
       "      <td>2519</td>\n",
       "      <td>2314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2436</td>\n",
       "      <td>1752</td>\n",
       "      <td>960</td>\n",
       "      <td>960</td>\n",
       "      <td>2436</td>\n",
       "      <td>1394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1892</td>\n",
       "      <td>121</td>\n",
       "      <td>186</td>\n",
       "      <td>1394</td>\n",
       "      <td>1892</td>\n",
       "      <td>2235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1552</td>\n",
       "      <td>1594</td>\n",
       "      <td>1394</td>\n",
       "      <td>186</td>\n",
       "      <td>1552</td>\n",
       "      <td>2519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1296</td>\n",
       "      <td>2498</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>121</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>294</td>\n",
       "      <td>1027</td>\n",
       "      <td>2436</td>\n",
       "      <td>2436</td>\n",
       "      <td>1296</td>\n",
       "      <td>1067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>814</td>\n",
       "      <td>814</td>\n",
       "      <td>814</td>\n",
       "      <td>814</td>\n",
       "      <td>814</td>\n",
       "      <td>814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1388</td>\n",
       "      <td>1693</td>\n",
       "      <td>1388</td>\n",
       "      <td>1388</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>180</td>\n",
       "      <td>1023</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>1388</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1992</td>\n",
       "      <td>2123</td>\n",
       "      <td>1693</td>\n",
       "      <td>1992</td>\n",
       "      <td>180</td>\n",
       "      <td>1388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1133</td>\n",
       "      <td>180</td>\n",
       "      <td>1023</td>\n",
       "      <td>1693</td>\n",
       "      <td>2492</td>\n",
       "      <td>2550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1578</td>\n",
       "      <td>1388</td>\n",
       "      <td>1992</td>\n",
       "      <td>1023</td>\n",
       "      <td>304</td>\n",
       "      <td>2484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2484</td>\n",
       "      <td>1378</td>\n",
       "      <td>2123</td>\n",
       "      <td>2123</td>\n",
       "      <td>2123</td>\n",
       "      <td>1235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1235</td>\n",
       "      <td>304</td>\n",
       "      <td>1133</td>\n",
       "      <td>1133</td>\n",
       "      <td>1790</td>\n",
       "      <td>2492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2550</td>\n",
       "      <td>1790</td>\n",
       "      <td>1578</td>\n",
       "      <td>1578</td>\n",
       "      <td>1023</td>\n",
       "      <td>1779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2492</td>\n",
       "      <td>2566</td>\n",
       "      <td>1378</td>\n",
       "      <td>1378</td>\n",
       "      <td>2484</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lexical semantic hybrid_v1 hybrid_v2 creative_v1 creative_v2\n",
       "0     1429     1398      1429      1429        1429         299\n",
       "1      299     2188      1398      1398         299        1887\n",
       "2      140     1429      2328       299        1689        1561\n",
       "3     1689     2328       410       140        2328        1295\n",
       "4     2328     2555      2188      1689        1735         773\n",
       "5     1398      333       299      2328        1887        2365\n",
       "6     1735      177       140      2188         140        1429\n",
       "7      410      410      1689      1735        1834        1342\n",
       "8     1887     2410      2555      2555        2477        1865\n",
       "9     2021     1125       333       410        2205        1999\n",
       "10    1594     1237      1237      1237        1594        1621\n",
       "11    1237     1722      1621      1621        1237        1237\n",
       "12    1621     1989      1594      1594        1621         861\n",
       "13     316     1621       510       316         316        1594\n",
       "14    2208      510      1722      2208        2208        1349\n",
       "15      15     1752      1989        15          15         510\n",
       "16    1067     1132       316       510         510        1258\n",
       "17     510     2073      2208      1067        1067         316\n",
       "18     122      861        15      1722         861        2208\n",
       "19    1793     1594      1752      1989         122          15\n",
       "20    2314     1237      2519      2314        2314         960\n",
       "21     303      359      2314       303         303         303\n",
       "22     960     2519      1237      2519         960         595\n",
       "23    1394      186       359      1237        1394        1892\n",
       "24    2519      137       303       359        2519        2314\n",
       "25    2436     1752       960       960        2436        1394\n",
       "26    1892      121       186      1394        1892        2235\n",
       "27    1552     1594      1394       186        1552        2519\n",
       "28    1296     2498       137       137         121        1185\n",
       "29     294     1027      2436      2436        1296        1067\n",
       "30     814      814       814       814         814         814\n",
       "31    1388     1693      1388      1388        1992        1992\n",
       "32     180     1023       180       180        1388         180\n",
       "33    1992     2123      1693      1992         180        1388\n",
       "34    1133      180      1023      1693        2492        2550\n",
       "35    1578     1388      1992      1023         304        2484\n",
       "36    2484     1378      2123      2123        2123        1235\n",
       "37    1235      304      1133      1133        1790        2492\n",
       "38    2550     1790      1578      1578        1023        1779\n",
       "39    2492     2566      1378      1378        2484         497"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def generate_exploded_df(search_results_df):\n",
    "#     exploded_search_results_df = pd.concat([search_results_df[col].explode().reset_index() for col in search_results_df.columns], axis=1)\n",
    "#     exploded_search_results_df = exploded_search_results_df.apply(lambda l: [doc_id for doc_id, _ in l])\n",
    "#     return exploded_search_results_df\n",
    "\n",
    "def generate_exploded_df(search_results_df):\n",
    "\n",
    "    df_doc_ids = search_results_df.applymap(\n",
    "        lambda lst: [doc_id for doc_id, _ in lst] if isinstance(lst, list) else []\n",
    "    )\n",
    "\n",
    "    exploded_df = pd.DataFrame({\n",
    "        col: df_doc_ids[col].explode().reset_index(drop=True)\n",
    "        for col in df_doc_ids.columns\n",
    "    })\n",
    "\n",
    "    return exploded_df\n",
    "\n",
    "def generate_found_docs_text_df(exploded_search_results_df, all_docs_df):\n",
    "    # Recupera os ids únicos dos documentos\n",
    "    documents_ids = set(exploded_search_results_df.to_numpy().flatten().tolist())\n",
    "\n",
    "    # Salva os textos e os ids dos documetnos que foram encontrados ems usas buscas\n",
    "    documents_df = all_docs_df[all_docs_df[\"doc_id\"].isin(documents_ids)][[\"Texto processado\", \"doc_id\"]]\n",
    "    return documents_df\n",
    "\n",
    "exploded_df = generate_exploded_df(search_results_df)\n",
    "found_docs_text_df = generate_found_docs_text_df(exploded_df, all_docs_df=df)\n",
    "\n",
    "def save_results_to_file(exploded_df: pd.DataFrame,\n",
    "                         found_docs_text_df: pd.DataFrame,\n",
    "                         exploded_df_save_filepath: str = \"data/search_results.csv\",\n",
    "                         found_docs_text_save_filepath: str = \"data/documents.csv\"):\n",
    "    exploded_df.to_csv(exploded_df_save_filepath)\n",
    "    found_docs_text_df.to_csv(found_docs_text_save_filepath)\n",
    "    print(\"Resultados das buscas salvos em 'data/search_results.csv'.\")\n",
    "    print(\"Documentos de interesse salvos em 'data/documents.csv'.\")\n",
    "    \n",
    "save_results_to_file(exploded_df, found_docs_text_df)\n",
    "exploded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>É falso que houve megaprotesto na Alemanha con...</td>\n",
       "      <td>circular rede social foto mostrar dezena carro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Estudo não aponta que carga viral de vacinados...</td>\n",
       "      <td>circular rede social estudo publicar revista c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>É antiga foto de manifestante em cima de carro...</td>\n",
       "      <td>circular rede social foto homem mascarar , seg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Diretor da Anvisa não pediu demissão e critico...</td>\n",
       "      <td>circular rede social diretor anvisa , agencia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Jogadoras da seleção de futebol não posaram in...</td>\n",
       "      <td>circular rede imagem dois jogadora selecao bra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>2565</td>\n",
       "      <td>Es falso que farmacias de Italia estén distrib...</td>\n",
       "      <td>circula en las rede sociales las farmacias en ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2566</th>\n",
       "      <td>2566</td>\n",
       "      <td>Foto viral que mostra Faixa de Gaza após bomba...</td>\n",
       "      <td>circular rede social foto mostrar cidade ceu c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2567</th>\n",
       "      <td>2567</td>\n",
       "      <td>É falso que STF afastou Bolsonaro do controle ...</td>\n",
       "      <td>circular rede social supremo tribunal federal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2568</th>\n",
       "      <td>2568</td>\n",
       "      <td>É falso que Magazine Luiza 'financia' fome dos...</td>\n",
       "      <td>em o comissao parlamentar inquerito ( cpi ) on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2569</th>\n",
       "      <td>2569</td>\n",
       "      <td>Como a nova faixa de isenção impactará no Impo...</td>\n",
       "      <td>em    fevereiro , receita federal anunciar mud...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2570 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      doc_id                                              title  \\\n",
       "0          0  É falso que houve megaprotesto na Alemanha con...   \n",
       "1          1  Estudo não aponta que carga viral de vacinados...   \n",
       "2          2  É antiga foto de manifestante em cima de carro...   \n",
       "3          3  Diretor da Anvisa não pediu demissão e critico...   \n",
       "4          4  Jogadoras da seleção de futebol não posaram in...   \n",
       "...      ...                                                ...   \n",
       "2565    2565  Es falso que farmacias de Italia estén distrib...   \n",
       "2566    2566  Foto viral que mostra Faixa de Gaza após bomba...   \n",
       "2567    2567  É falso que STF afastou Bolsonaro do controle ...   \n",
       "2568    2568  É falso que Magazine Luiza 'financia' fome dos...   \n",
       "2569    2569  Como a nova faixa de isenção impactará no Impo...   \n",
       "\n",
       "                                                content  \n",
       "0     circular rede social foto mostrar dezena carro...  \n",
       "1     circular rede social estudo publicar revista c...  \n",
       "2     circular rede social foto homem mascarar , seg...  \n",
       "3     circular rede social diretor anvisa , agencia ...  \n",
       "4     circular rede imagem dois jogadora selecao bra...  \n",
       "...                                                 ...  \n",
       "2565  circula en las rede sociales las farmacias en ...  \n",
       "2566  circular rede social foto mostrar cidade ceu c...  \n",
       "2567  circular rede social supremo tribunal federal ...  \n",
       "2568  em o comissao parlamentar inquerito ( cpi ) on...  \n",
       "2569  em    fevereiro , receita federal anunciar mud...  \n",
       "\n",
       "[2570 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_id_map(all_docs_df, output_csv_path):\n",
    "    \"\"\"\n",
    "    Exports a stable mapping of ALL documents used in Elasticsearch.\n",
    "\n",
    "    The doc_id values will exactly match those returned by search results.\n",
    "    \"\"\"\n",
    "\n",
    "    required_cols = {\"doc_id\", \"Título\", \"Texto processado\"}\n",
    "    missing = required_cols - set(all_docs_df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    id_map_df = (\n",
    "        all_docs_df[[\"doc_id\", \"Título\", \"Texto processado\"]]\n",
    "        .rename(columns={\n",
    "            \"Título\": \"title\",\n",
    "            \"Texto processado\": \"content\"\n",
    "        })\n",
    "        .sort_values(\"doc_id\")   # optional, but good for reproducibility\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    id_map_df.to_csv(output_csv_path, sep=\";\", index=False)\n",
    "\n",
    "    return id_map_df\n",
    "\n",
    "generate_id_map(df, \"data/id_map.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lab-elastic)",
   "language": "python",
   "name": "lab-elastic-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
