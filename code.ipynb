{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratório Elastic Search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "# Código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to ./nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to ./nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import warnings\n",
    "import subprocess\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import OrderedDict\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import sklearn\n",
    "\n",
    "# Download localmente\n",
    "nltk.data.path.append('./nltk_data')\n",
    "nltk.download('punkt_tab', download_dir='./nltk_data')\n",
    "nltk.download('stopwords',download_dir='./nltk_data')\n",
    "\n",
    "import spacy\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rode no terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/python: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download pt_core_news_sm #se der errado rodar isso no terminal: python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo systemctl start docker.service && docker compose up -d #se der errado rodar isso no terminal: sudo systemctl start docker && docker compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 1: Baixar dados do Lupa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: Arquivo extraído com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Base de dados de notícias da Lupa\n",
    "url = \"https://docs.google.com/uc?export=download&confirm=t&id=1W067Md2EbvVzW1ufzFg17Hf7Y9cCZxxr\"\n",
    "filename = \"articles_lupa_lab_elasticsearch.zip\"\n",
    "data_path = \"data\"\n",
    "zip_file_path = f\"{data_path}/{filename}\"\n",
    "\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "# Baixa o zip\n",
    "with open(zip_file_path, \"wb\") as f:\n",
    "    f.write(requests.get(url, allow_redirects=True).content)\n",
    "\n",
    "# Extrai o csv do zip\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_path)\n",
    "    \n",
    "output_file = f\"{data_path}/articles_lupa.csv\"\n",
    "assert os.path.exists(output_file)\n",
    "print(\"OK: Arquivo extraído com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 2: Pré-processar os dados e gerar embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementações de pré-processamentos de texto. Modifiquem, adicionem, removam conforme necessário.\n",
    "class Preprocessors:\n",
    "    STOPWORDS = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.spacy_nlp = spacy.load(\"pt_core_news_sm\") # Utiliza para lematização\n",
    "        \n",
    "    # Remove stopwords do português\n",
    "    def remove_stopwords(self, text):\n",
    "        # Tokeniza as palavras\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove as stop words\n",
    "        tokens = [word for word in tokens if word not in self.STOPWORDS]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    # Realiza a lematização\n",
    "    def lemma(self, text):\n",
    "        return \" \".join([token.lemma_ for token in self.spacy_nlp(text)])\n",
    "    \n",
    "    # Realiza a stemização\n",
    "    def porter_stemmer(self, text):\n",
    "        # Tokeniza as palavras\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        for index in range(len(tokens)):\n",
    "            # Realiza a stemização\n",
    "            stem_word = self.stemmer.stem(tokens[index])\n",
    "            tokens[index] = stem_word\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    # Transforma o texto em lower case\n",
    "    def lower_case(self, str):\n",
    "        return str.lower()\n",
    "\n",
    "    # Remove urls com regex\n",
    "    def remove_urls(self, text):\n",
    "        url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "        without_urls = re.sub(pattern=url_pattern, repl=' ', string=text)\n",
    "        return without_urls\n",
    "\n",
    "    # Remove números com regex\n",
    "    def remove_numbers(self, text):\n",
    "        number_pattern = r'\\d+'\n",
    "        without_number = re.sub(pattern=number_pattern,\n",
    "    repl=\" \", string=text)\n",
    "        return without_number\n",
    "\n",
    "    # Converte caracteres acentuados para sua versão ASCII\n",
    "    def accented_to_ascii(self, text):\n",
    "        text = unidecode.unidecode(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "def extrair_features(texto):\n",
    "    doc = nlp(str(texto))\n",
    "    features = {\"PER\": [], \"ORG\": [], \"LOC\": [], \"NOUNS\": [], \"PROPN\": []}\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        t = ent.text.strip().lower() # Normaliza para comparar\n",
    "        t_original = ent.text.strip()\n",
    "        \n",
    "        # Filtros de qualidade para evitar ruído \n",
    "        if len(t) < 3: continue\n",
    "        if t.replace(\"/\", \"\").replace(\"-\", \"\").isdigit(): continue\n",
    "        \n",
    "        if ent.label_ in features:\n",
    "            features[ent.label_].append(t_original)\n",
    "    \n",
    "    for token in doc:\n",
    "        # Ignora pontuação e palavras curtas\n",
    "        if token.is_punct or len(token.text) < 3: continue\n",
    "        \n",
    "        palavra = token.text.strip().lower()\n",
    "        \n",
    "        # Se for substantivo (ex: vacina, autismo, cobrança)\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            features[\"NOUNS\"].append(palavra)\n",
    "            \n",
    "        elif token.pos_ == \"PROPN\":\n",
    "            features[\"PROPN\"].append(palavra)\n",
    "\n",
    "            \n",
    "    # Retorna listas únicas\n",
    "    return {k: list(set(v)) for k, v in features.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando Embeddings e Extraindo Entidades... (Isso pode demorar um pouco)\n",
      "Geração de embeddings finalizada.\n"
     ]
    }
   ],
   "source": [
    "# Carregar o modelo gerador de embeddings\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Caminho para salvar o dataframe de notícias\n",
    "data_df_path = \"data/data_df.pkl\"\n",
    "\n",
    "# Selecione diferentes pré-processamentos\n",
    "# Exemplo:\n",
    "\n",
    "preprocessor = Preprocessors()\n",
    "preprocessing_steps = [\n",
    "    preprocessor.remove_urls,\n",
    "    preprocessor.remove_stopwords,\n",
    "    preprocessor.remove_numbers,\n",
    "    preprocessor.lemma,\n",
    "    preprocessor.accented_to_ascii,\n",
    "    preprocessor.lower_case,\n",
    "    #preprocessor.porter_stemmer\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "RECREATE_DF = True\n",
    "\n",
    "# Cria o data frame se ele já existir ou se a variável RECREATE_INDEX for verdadeira\n",
    "# Ou (exclusivo) carrega o dataframe salvo\n",
    "if not os.path.exists(data_df_path) or RECREATE_DF:    \n",
    "    df = pd.read_csv(output_file, sep=\";\")[[\"Título\", \"Texto\", \"Data de Publicação\"]]\n",
    "    df[\"Data de Publicação\"] = df[\"Data de Publicação\"].apply(lambda str_date: datetime.strptime(str_date.split(\" - \")[0], \"%d.%m.%Y\"))\n",
    "    df.sort_values(\"Data de Publicação\", inplace=True, ascending=False)\n",
    "\n",
    "    df = df.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    df[\"Embeddings\"] = [None] * len(df)\n",
    "    df[\"doc_id\"] = df.reset_index(drop=True).index\n",
    "\n",
    "    df[\"entidades_per\"] = [[] for _ in range(len(df))]\n",
    "    df[\"entidades_org\"] = [[] for _ in range(len(df))]\n",
    "    df[\"entidades_loc\"] = [[] for _ in range(len(df))]\n",
    "    df[\"entidades_noun\"] = [[] for _ in range(len(df))]\n",
    "    df[\"entidades_propn\"] = [[] for _ in range(len(df))]\n",
    "    \n",
    "    print(\"Gerando Embeddings e Extraindo Entidades... (Isso pode demorar um pouco)\")\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        texto_completo = row[\"Texto\"].strip() + \"\\n\" + row[\"Título\"].strip()\n",
    "\n",
    "        ents = extrair_features(texto_completo)\n",
    "\n",
    "        #Colocando as entidades\n",
    "        df.at[i, \"entidades_per\"] = ents[\"PER\"]\n",
    "        df.at[i, \"entidades_org\"] = ents[\"ORG\"]\n",
    "        df.at[i, \"entidades_loc\"] = ents[\"LOC\"]\n",
    "        df.at[i, \"entidades_noun\"] = ents[\"NOUNS\"]\n",
    "        df.at[i, \"entidades_propn\"] = ents[\"PROPN\"]\n",
    "        \n",
    "        df.at[i, \"Texto completo\"] = texto_completo\n",
    "        texto_processado = texto_completo\n",
    "        for preprocessing_step in preprocessing_steps:\n",
    "            texto_processado = preprocessing_step(texto_processado)\n",
    "        \n",
    "        df.at[i, \"Texto processado\"] = texto_processado\n",
    "        embeddings = model.encode(texto_completo).tolist()\n",
    "        df.at[i, \"Embeddings\"] = embeddings\n",
    "        \n",
    "    print(\"Geração de embeddings finalizada.\")\n",
    "    \n",
    "    with open(data_df_path, \"wb\") as f:\n",
    "        df.to_pickle(f)\n",
    "else:\n",
    "    with open(data_df_path, \"rb\") as f:\n",
    "        df = pd.read_pickle(f)\n",
    "    print(\"Dataframe carregado de arquivo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 3: Indexar dados no ElasticSearch (Lembrem-se de reindexar os dados se os pré-processamentos mudarem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    hosts = [{'host': \"localhost\", 'port': 9200, \"scheme\": \"https\"}],\n",
    "    basic_auth=(\"elastic\",\"elastic\"),\n",
    "    verify_certs = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice 'verificacoes_lupa' deletado.\n",
      "Índice 'verificacoes_lupa' criado.\n",
      "Índice preenchido.\n",
      "Indexação finalizada.\n"
     ]
    }
   ],
   "source": [
    "RECREATE_INDEX = True\n",
    "\n",
    "index_name = \"verificacoes_lupa\"\n",
    "\n",
    "# Se a flag for True e se o índice existir, ele é deletado\n",
    "if RECREATE_INDEX and es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "    print(f\"Índice '{index_name}' deletado.\")\n",
    "\n",
    "# Cria o índice e popula com os dados\n",
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(index=index_name, mappings={\n",
    "        \"properties\": {\n",
    "            \"doc_id\": {\"type\": \"integer\"},\n",
    "            \"full_text\": {\"type\": \"text\"},\n",
    "            \"processed_text\": {\"type\": \"text\"},\n",
    "            \"embeddings\": {\"type\": \"dense_vector\", \"dims\": 384},\n",
    "            \"entidades_per\": {\"type\": \"keyword\"},\n",
    "            \"entidades_org\": {\"type\": \"keyword\"},\n",
    "            \"entidades_loc\": {\"type\": \"keyword\"},\n",
    "            \"entidades_noun\": {\"type\": \"keyword\"},\n",
    "            \"entidades_propn\": {\"type\": \"keyword\"},\n",
    "        }\n",
    "    })\n",
    "    print(f\"Índice '{index_name}' criado.\")\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        es.index(index=index_name, id=row[\"doc_id\"], body={\n",
    "            \"doc_id\": row[\"doc_id\"],\n",
    "            \"full_text\": row[\"Texto completo\"],\n",
    "            \"processed_text\": row[\"Texto processado\"],\n",
    "            \"embeddings\": row[\"Embeddings\"],\n",
    "            \"entidades_per\": row[\"entidades_per\"],\n",
    "            \"entidades_org\": row[\"entidades_org\"],\n",
    "            \"entidades_loc\": row[\"entidades_loc\"],\n",
    "            \"entidades_noun\": row[\"entidades_noun\"],\n",
    "            \"entidades_propn\": row[\"entidades_propn\"],\n",
    "        })\n",
    "    print(\"Índice preenchido.\")\n",
    "\n",
    "print(\"Indexação finalizada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estas serão as queries QF1 e QF2\n",
    "with open(\"data/queries_fixadas.txt\", \"r\") as f:\n",
    "    queries_fixadas = [line.strip() for line in f.readlines()]\n",
    "    assert len(queries_fixadas) == 2\n",
    "    QF1 = queries_fixadas[0]\n",
    "    QF2 = queries_fixadas[1]\n",
    "    \n",
    "# Preencha aqui as queries do grupo\n",
    "QP1 = \"vacina causa autismo\"\n",
    "QP2 = \"massacre em Israel\"\n",
    "\n",
    "queries = OrderedDict()\n",
    "queries[\"QF1\"] = QF1\n",
    "queries[\"QF2\"] = QF2\n",
    "queries[\"QP1\"] = QP1\n",
    "queries[\"QP2\"] = QP2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Busca Léxica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação de busca esparsa (léxica) com BM25\n",
    "def lexical_search(queries: dict[str, str]):\n",
    "    lexical_results = {}\n",
    "    for query_id, query in queries.items():\n",
    "        \n",
    "        # Pré-processa os dados\n",
    "        for preprocessing_step in preprocessing_steps:\n",
    "            query = preprocessing_step(query)\n",
    "        \n",
    "        search_query = {\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"processed_text\": query\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Realiza a busca\n",
    "        response = es.search(index=index_name, body=search_query)\n",
    "        \n",
    "        hits_results = []\n",
    "        # Recupera os resultados\n",
    "        for hit in response[\"hits\"][\"hits\"]:\n",
    "            hits_results.append((hit[\"_source\"][\"doc_id\"], hit[\"_score\"]))\n",
    "        lexical_results[query_id] = hits_results\n",
    "        \n",
    "    return lexical_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Busca Semântica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza busca semântica (densa) com KNN exato\n",
    "def semantic_search(queries: dict[str, str]):\n",
    "    semantic_results = {}\n",
    "    \n",
    "    for query_id, query in queries.items():\n",
    "        # Aplica todos os pré-processamentos aos dados\n",
    "        for preprocessing_step in preprocessing_steps:\n",
    "            query = preprocessing_step(query)\n",
    "            \n",
    "        query_vector = model.encode(query).tolist()\n",
    "        \n",
    "        \n",
    "        search_query = {\n",
    "            \"query\": {\n",
    "                \"script_score\": {\n",
    "                    \"query\": {\"match_all\": {}},\n",
    "                    \"script\": {\n",
    "                        \"source\": \"cosineSimilarity(params.query_vector, 'embeddings') + 1.0\",\n",
    "                        \"params\": {\"query_vector\": query_vector}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Realiza a busca\n",
    "        response = es.search(index=index_name, body=search_query)\n",
    "        \n",
    "        hits_results = []\n",
    "        # Recupera top 10 resultados\n",
    "        for hit in response[\"hits\"][\"hits\"]:\n",
    "            hits_results.append((hit[\"_source\"][\"doc_id\"], hit[\"_score\"]))\n",
    "            \n",
    "        semantic_results[query_id] = hits_results\n",
    "\n",
    "    return semantic_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Busca Híbrida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca híbrida ou RRF. Implemente sua solução aqui. Você pode realizar as duas buscas anteriores (léxica e semântica) como base para formar a busca híbrida.\n",
    "def hybrid_search(queries: dict[str, str]):\n",
    "    top_k = 10\n",
    "\n",
    "    lexical_results = lexical_search(queries)\n",
    "    semantic_results = semantic_search(queries)\n",
    "\n",
    "    alpha = 0.55\n",
    "    beta = 0.35\n",
    "\n",
    "    k_rrf = 60\n",
    "    w_rrf = 0.25\n",
    "\n",
    "    relevance_documents = {}\n",
    "\n",
    "    # Normalizando os scores [0,1]\n",
    "    def normalize_score_map(score_map):\n",
    "        if not score_map:\n",
    "            return {}\n",
    "\n",
    "        values = list(score_map.values())\n",
    "        mn, mx = min(values), max(values)\n",
    "\n",
    "        if mx == mn:\n",
    "            return {doc: 1.0 for doc in score_map}\n",
    "\n",
    "        return {\n",
    "            doc: (score - mn) / (mx - mn)\n",
    "            for doc, score in score_map.items()\n",
    "        }\n",
    "\n",
    "    for qid in queries.keys():\n",
    "        lex_list = lexical_results.get(qid, [])\n",
    "        sem_list = semantic_results.get(qid, [])\n",
    "\n",
    "        lexical_rank = {\n",
    "            doc_id: rank + 1\n",
    "            for rank, (doc_id, _) in enumerate(lex_list)\n",
    "        }\n",
    "\n",
    "        semantic_rank = {\n",
    "            doc_id: rank + 1\n",
    "            for rank, (doc_id, _) in enumerate(sem_list)\n",
    "        }\n",
    "\n",
    "        lexical_score = {\n",
    "            doc_id: score\n",
    "            for doc_id, score in lex_list\n",
    "        }\n",
    "\n",
    "        semantic_score = {\n",
    "            doc_id: score\n",
    "            for doc_id, score in sem_list\n",
    "        }\n",
    "\n",
    "        norm_lex = normalize_score_map(lexical_score)\n",
    "        norm_sem = normalize_score_map(semantic_score)\n",
    "\n",
    "        candidate_docs = set(lexical_rank) | set(semantic_rank)\n",
    "\n",
    "        scored = []\n",
    "\n",
    "        for doc_id in candidate_docs:\n",
    "            # RRF baseado nos ranks\n",
    "            rrf_lex = (\n",
    "                1.0 / (k_rrf + lexical_rank[doc_id])\n",
    "                if doc_id in lexical_rank\n",
    "                else 0.0\n",
    "            )\n",
    "\n",
    "            rrf_sem = (\n",
    "                1.0 / (k_rrf + semantic_rank[doc_id])\n",
    "                if doc_id in semantic_rank\n",
    "                else 0.0\n",
    "            )\n",
    "\n",
    "            rrf_sum = rrf_lex + rrf_sem\n",
    "\n",
    "            nlex = norm_lex.get(doc_id, 0.0)\n",
    "            nsem = norm_sem.get(doc_id, 0.0)\n",
    "\n",
    "            # Juntando os scores --> scores normalizados + RRF\n",
    "            combined = (\n",
    "                (alpha * nlex)\n",
    "                + (beta * nsem)\n",
    "                + (w_rrf * rrf_sum)\n",
    "            )\n",
    "\n",
    "            scored.append((doc_id, combined))\n",
    "\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        relevance_documents[qid] = scored[:top_k]\n",
    "\n",
    "    for doc_id, score in relevance_documents.get(\"QF1\", []):\n",
    "        print(f\"DOC_ID={doc_id} SCORE={score}\")\n",
    "\n",
    "    return relevance_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Busca Criativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemente sua própria estratégia de busca, podendo ela ser esparsa, densa ou híbrida. Implemente algo como \"more_like_this\", \"BM35\", \"fuzzy\" etc.\n",
    "def creative_search(queries: dict[str, str]):\n",
    "    resultados_queries = {}\n",
    "\n",
    "    for query_id, query in queries.items():\n",
    "        ents_query = extrair_features(query)\n",
    "\n",
    "        #Extrai entidades das consultas do usuário\n",
    "        q_per = ents_query[\"PER\"]\n",
    "        q_org = ents_query[\"ORG\"]\n",
    "        q_loc = ents_query[\"LOC\"]\n",
    "        q_noun = ents_query[\"NOUNS\"]\n",
    "        q_propn = ents_query[\"PROPN\"]\n",
    "\n",
    "        for processor in preprocessing_steps:\n",
    "            query = processor(query)\n",
    "\n",
    "        # Gera vetor da query\n",
    "        query_vector = model.encode(query).tolist()\n",
    "\n",
    "        # Monta a query usando uma combinação de BM25, KNN e boost por entidades\n",
    "        body = {\n",
    "            \"size\": 10,\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    # \"Must\", o documento deve ter isso\n",
    "                    \"must\": [\n",
    "                        {\n",
    "                            \"bool\": {\n",
    "                                \"should\": [\n",
    "                                    {\"match\": {\"processed_text\": query}},  # BM25\n",
    "                                    {\n",
    "                                        \"knn\": {\n",
    "                                            \"field\": \"embeddings\",\n",
    "                                            \"query_vector\": query_vector,\n",
    "                                            \"k\": 10,\n",
    "                                            \"boost\": 0.5,\n",
    "                                        }\n",
    "                                    },  # Vetorial\n",
    "                                ]\n",
    "                            }\n",
    "                        }\n",
    "                    ],\n",
    "                    # \"Should\", Se bater a entidade, a nota sobe.\n",
    "                    \"should\": [\n",
    "                        {\"terms\": {\"entidades_per\": q_per, \"boost\": 5.0}},\n",
    "                        {\"terms\": {\"entidades_org\": q_org, \"boost\": 5.0}},\n",
    "                        {\"terms\": {\"entidades_loc\": q_loc, \"boost\": 4.0}},\n",
    "                        {\"terms\": {\"entidades_noun\": q_noun, \"boost\": 4.0}},\n",
    "                        {\"terms\": {\"entidades_propn\": q_propn, \"boost\": 3.5}},\n",
    "                    ],\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Executa a busca\n",
    "        res = es.search(index=index_name, body=body)\n",
    "\n",
    "        # Formata retorno apenas com doc_id e score\n",
    "        resultados = [\n",
    "            (hit[\"_source\"][\"doc_id\"], hit[\"_score\"])\n",
    "            for hit in res[\"hits\"][\"hits\"]\n",
    "        ]\n",
    "\n",
    "        resultados_queries[query_id] = resultados\n",
    "\n",
    "    return resultados_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execução das buscas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOC_ID=1429 SCORE=0.6681443123171364\n",
      "DOC_ID=1398 SCORE=0.5795347822356685\n",
      "DOC_ID=299 SCORE=0.4748216303239906\n",
      "DOC_ID=140 SCORE=0.4636719694802592\n",
      "DOC_ID=1689 SCORE=0.3878107359364247\n",
      "DOC_ID=2328 SCORE=0.3732524832202891\n",
      "DOC_ID=2188 SCORE=0.15850696759012656\n",
      "DOC_ID=1735 SCORE=0.13828202320633787\n",
      "DOC_ID=410 SCORE=0.08993281881100597\n",
      "DOC_ID=333 SCORE=0.06593411183498271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lexical': {'QF1': [(1429, 17.123802),\n",
       "   (299, 16.410084),\n",
       "   (140, 16.310198),\n",
       "   (1689, 15.627218),\n",
       "   (2328, 14.4873905),\n",
       "   (1398, 14.16523),\n",
       "   (1735, 13.380445),\n",
       "   (1887, 12.564361),\n",
       "   (410, 12.548052),\n",
       "   (2021, 12.168092)],\n",
       "  'QF2': [(1594, 12.909736),\n",
       "   (1237, 12.770021),\n",
       "   (1621, 12.684045),\n",
       "   (316, 12.655595),\n",
       "   (2208, 12.639837),\n",
       "   (15, 12.629146),\n",
       "   (1067, 12.198853),\n",
       "   (510, 11.993293),\n",
       "   (122, 11.719714),\n",
       "   (1793, 11.451164)],\n",
       "  'QP1': [(2314, 11.014164),\n",
       "   (303, 10.45681),\n",
       "   (960, 9.476071),\n",
       "   (1394, 9.310348),\n",
       "   (1892, 8.326734),\n",
       "   (1552, 8.277965),\n",
       "   (1296, 7.771367),\n",
       "   (294, 7.661727),\n",
       "   (121, 7.396516),\n",
       "   (1629, 7.331139)],\n",
       "  'QP2': [(814, 15.423179),\n",
       "   (1388, 13.6003685),\n",
       "   (180, 12.907921),\n",
       "   (1992, 11.127016),\n",
       "   (1133, 8.7520685),\n",
       "   (1578, 8.497414),\n",
       "   (1235, 7.350944),\n",
       "   (497, 7.171792),\n",
       "   (1779, 7.164755),\n",
       "   (981, 7.1247425)]},\n",
       " 'semantic': {'QF1': [(1398, 1.6246119),\n",
       "   (2188, 1.5283167),\n",
       "   (1429, 1.5064514),\n",
       "   (2328, 1.505476),\n",
       "   (333, 1.4828167),\n",
       "   (177, 1.4721812),\n",
       "   (410, 1.47214),\n",
       "   (1125, 1.4546205),\n",
       "   (1976, 1.4526354),\n",
       "   (1690, 1.4522387)],\n",
       "  'QF2': [(1237, 1.8076564),\n",
       "   (1722, 1.7621573),\n",
       "   (1989, 1.7520217),\n",
       "   (1621, 1.7430636),\n",
       "   (510, 1.7271051),\n",
       "   (1752, 1.7106292),\n",
       "   (1132, 1.7047325),\n",
       "   (2073, 1.7021568),\n",
       "   (861, 1.701055),\n",
       "   (1594, 1.7007091)],\n",
       "  'QP1': [(1237, 1.5580077),\n",
       "   (359, 1.5565056),\n",
       "   (186, 1.5387578),\n",
       "   (137, 1.530966),\n",
       "   (1752, 1.519625),\n",
       "   (121, 1.5192797),\n",
       "   (1594, 1.5181099),\n",
       "   (1027, 1.514234),\n",
       "   (2149, 1.5136712),\n",
       "   (921, 1.512231)],\n",
       "  'QP2': [(814, 1.7237177),\n",
       "   (1693, 1.6455504),\n",
       "   (1023, 1.6314373),\n",
       "   (2123, 1.6291357),\n",
       "   (180, 1.6178629),\n",
       "   (1388, 1.6123213),\n",
       "   (1378, 1.6045778),\n",
       "   (304, 1.590568),\n",
       "   (1790, 1.5806785),\n",
       "   (1199, 1.5444645)]},\n",
       " 'hybrid': {'QF1': [(1429, 0.6681443123171364),\n",
       "   (1398, 0.5795347822356685),\n",
       "   (299, 0.4748216303239906),\n",
       "   (140, 0.4636719694802592),\n",
       "   (1689, 0.3878107359364247),\n",
       "   (2328, 0.3732524832202891),\n",
       "   (2188, 0.15850696759012656),\n",
       "   (1735, 0.13828202320633787),\n",
       "   (410, 0.08993281881100597),\n",
       "   (333, 0.06593411183498271)],\n",
       "  'QF2': [(1237, 0.8554467265298098),\n",
       "   (1621, 0.6113817094232844),\n",
       "   (1594, 0.5576697892271664),\n",
       "   (316, 0.4580744706980524),\n",
       "   (2208, 0.452072329859405),\n",
       "   (15, 0.44798268027865196),\n",
       "   (510, 0.2983338510669949),\n",
       "   (1067, 0.2856707675972256),\n",
       "   (1722, 0.20513008849127745),\n",
       "   (1989, 0.1718959155361477)],\n",
       "  'QP1': [(2314, 0.5540983606557377),\n",
       "   (303, 0.4708004852690561),\n",
       "   (1237, 0.3540983606557377),\n",
       "   (359, 0.3425474852434071),\n",
       "   (960, 0.32427903111475714),\n",
       "   (1394, 0.299468987152205),\n",
       "   (186, 0.206787155287046),\n",
       "   (1892, 0.15252210635801572),\n",
       "   (137, 0.14715052055685493),\n",
       "   (1552, 0.14518097277990982)],\n",
       "  'QP2': [(814, 0.9081967213114754),\n",
       "   (1388, 0.5695022100940842),\n",
       "   (180, 0.5344229858400473),\n",
       "   (1992, 0.2691671126938339),\n",
       "   (1693, 0.20140706085743731),\n",
       "   (1023, 0.173786589149997),\n",
       "   (2123, 0.16923060683156574),\n",
       "   (1378, 0.12110528695655423),\n",
       "   (1133, 0.11170132632352354),\n",
       "   (1578, 0.09476517613780724)]},\n",
       " 'creative': {'QF1': [(1429, 21.499966),\n",
       "   (299, 20.410084),\n",
       "   (1689, 19.627218),\n",
       "   (2328, 18.86467),\n",
       "   (1735, 17.380444),\n",
       "   (1887, 16.926271),\n",
       "   (140, 16.310198),\n",
       "   (1834, 15.829229),\n",
       "   (2205, 14.880583),\n",
       "   (1295, 14.714508)],\n",
       "  'QF2': [(1594, 17.334835),\n",
       "   (1237, 17.221786),\n",
       "   (1621, 17.120228),\n",
       "   (316, 16.655594),\n",
       "   (2208, 16.639837),\n",
       "   (15, 16.629147),\n",
       "   (510, 16.42551),\n",
       "   (1067, 16.198853),\n",
       "   (861, 15.772586),\n",
       "   (122, 15.719714)],\n",
       "  'QP1': [(2314, 15.014164),\n",
       "   (303, 14.45681),\n",
       "   (960, 13.476071),\n",
       "   (1394, 13.310348),\n",
       "   (1892, 12.326734),\n",
       "   (1552, 12.277965),\n",
       "   (121, 11.777568),\n",
       "   (1296, 11.771367),\n",
       "   (294, 11.661727),\n",
       "   (1629, 11.331139)],\n",
       "  'QP2': [(814, 27.355742),\n",
       "   (1992, 22.627016),\n",
       "   (1388, 21.504845),\n",
       "   (180, 20.813652),\n",
       "   (1790, 14.894123),\n",
       "   (1023, 14.860209),\n",
       "   (1693, 14.8523035),\n",
       "   (1235, 14.8509445),\n",
       "   (1147, 14.696243),\n",
       "   (497, 14.671792)]}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_functions = [\n",
    "    (\"lexical\", lexical_search),\n",
    "    (\"semantic\", semantic_search),\n",
    "    (\"hybrid\", hybrid_search),\n",
    "    (\"creative\", creative_search),\n",
    "]\n",
    "\n",
    "def run_all_searches(queries: dict[str, str]):\n",
    "    all_search_results = {}\n",
    "    for search_name, search_function in search_functions:\n",
    "        results = search_function(queries)\n",
    "        all_search_results[search_name] = results\n",
    "    return all_search_results\n",
    "\n",
    "run_all_searches(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analise os resultados da busca e aprimore a busca!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOC_ID=1429 SCORE=0.664782689672494\n",
      "DOC_ID=1398 SCORE=0.5820902340866436\n",
      "DOC_ID=299 SCORE=0.47985574302555195\n",
      "DOC_ID=140 SCORE=0.47975423370538595\n",
      "DOC_ID=1689 SCORE=0.3972543367419193\n",
      "DOC_ID=2328 SCORE=0.3761039364731097\n",
      "DOC_ID=2188 SCORE=0.15576740466604982\n",
      "DOC_ID=1735 SCORE=0.14045575524036294\n",
      "DOC_ID=2555 SCORE=0.08485548725949116\n",
      "DOC_ID=410 SCORE=0.08320631565342534\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexical</th>\n",
       "      <th>semantic</th>\n",
       "      <th>hybrid</th>\n",
       "      <th>creative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QF1</th>\n",
       "      <td>[(1429, 17.123802), (299, 16.410084), (140, 16...</td>\n",
       "      <td>[(1398, 1.6246119), (2188, 1.5283167), (1429, ...</td>\n",
       "      <td>[(1429, 0.664782689672494), (1398, 0.582090234...</td>\n",
       "      <td>[(1429, 21.19925), (299, 20.192177), (1689, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QF2</th>\n",
       "      <td>[(1594, 12.909736), (1237, 12.770021), (1621, ...</td>\n",
       "      <td>[(1237, 1.8076564), (1722, 1.7621573), (1989, ...</td>\n",
       "      <td>[(1237, 0.856347014966595), (1621, 0.614106329...</td>\n",
       "      <td>[(1594, 17.421288), (1237, 17.308786), (1621, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP1</th>\n",
       "      <td>[(2314, 11.014164), (303, 10.45681), (960, 9.4...</td>\n",
       "      <td>[(1237, 1.5580077), (359, 1.5565056), (2519, 1...</td>\n",
       "      <td>[(2314, 0.5540983606557377), (303, 0.463028840...</td>\n",
       "      <td>[(2314, 15.017654), (303, 14.456556), (960, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QP2</th>\n",
       "      <td>[(814, 15.423179), (1388, 13.6003685), (180, 1...</td>\n",
       "      <td>[(814, 1.7237177), (1693, 1.6455504), (1023, 1...</td>\n",
       "      <td>[(814, 0.9081967213114754), (1388, 0.535172318...</td>\n",
       "      <td>[(814, 27.281761), (1992, 22.548485), (1388, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               lexical  \\\n",
       "QF1  [(1429, 17.123802), (299, 16.410084), (140, 16...   \n",
       "QF2  [(1594, 12.909736), (1237, 12.770021), (1621, ...   \n",
       "QP1  [(2314, 11.014164), (303, 10.45681), (960, 9.4...   \n",
       "QP2  [(814, 15.423179), (1388, 13.6003685), (180, 1...   \n",
       "\n",
       "                                              semantic  \\\n",
       "QF1  [(1398, 1.6246119), (2188, 1.5283167), (1429, ...   \n",
       "QF2  [(1237, 1.8076564), (1722, 1.7621573), (1989, ...   \n",
       "QP1  [(1237, 1.5580077), (359, 1.5565056), (2519, 1...   \n",
       "QP2  [(814, 1.7237177), (1693, 1.6455504), (1023, 1...   \n",
       "\n",
       "                                                hybrid  \\\n",
       "QF1  [(1429, 0.664782689672494), (1398, 0.582090234...   \n",
       "QF2  [(1237, 0.856347014966595), (1621, 0.614106329...   \n",
       "QP1  [(2314, 0.5540983606557377), (303, 0.463028840...   \n",
       "QP2  [(814, 0.9081967213114754), (1388, 0.535172318...   \n",
       "\n",
       "                                              creative  \n",
       "QF1  [(1429, 21.19925), (299, 20.192177), (1689, 19...  \n",
       "QF2  [(1594, 17.421288), (1237, 17.308786), (1621, ...  \n",
       "QP1  [(2314, 15.017654), (303, 14.456556), (960, 13...  \n",
       "QP2  [(814, 27.281761), (1992, 22.548485), (1388, 2...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_search_results = run_all_searches(queries)\n",
    "search_results_df = pd.DataFrame(all_search_results)\n",
    "search_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados das buscas salvos em 'data/search_results.csv'.\n",
      "Documentos de interesse salvos em 'data/documents.csv'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexical</th>\n",
       "      <th>semantic</th>\n",
       "      <th>hybrid</th>\n",
       "      <th>creative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1429</td>\n",
       "      <td>1398</td>\n",
       "      <td>1429</td>\n",
       "      <td>1429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>299</td>\n",
       "      <td>2188</td>\n",
       "      <td>1398</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140</td>\n",
       "      <td>1429</td>\n",
       "      <td>299</td>\n",
       "      <td>1689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1689</td>\n",
       "      <td>2328</td>\n",
       "      <td>140</td>\n",
       "      <td>2328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2328</td>\n",
       "      <td>2555</td>\n",
       "      <td>1689</td>\n",
       "      <td>1735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1398</td>\n",
       "      <td>333</td>\n",
       "      <td>2328</td>\n",
       "      <td>1887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1735</td>\n",
       "      <td>177</td>\n",
       "      <td>2188</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1887</td>\n",
       "      <td>410</td>\n",
       "      <td>1735</td>\n",
       "      <td>1834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>410</td>\n",
       "      <td>2410</td>\n",
       "      <td>2555</td>\n",
       "      <td>2477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021</td>\n",
       "      <td>1125</td>\n",
       "      <td>410</td>\n",
       "      <td>2205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1594</td>\n",
       "      <td>1237</td>\n",
       "      <td>1237</td>\n",
       "      <td>1594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1237</td>\n",
       "      <td>1722</td>\n",
       "      <td>1621</td>\n",
       "      <td>1237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1621</td>\n",
       "      <td>1989</td>\n",
       "      <td>1594</td>\n",
       "      <td>1621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>316</td>\n",
       "      <td>1621</td>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2208</td>\n",
       "      <td>510</td>\n",
       "      <td>2208</td>\n",
       "      <td>2208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>1752</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1067</td>\n",
       "      <td>1132</td>\n",
       "      <td>510</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>510</td>\n",
       "      <td>2073</td>\n",
       "      <td>1067</td>\n",
       "      <td>1067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>122</td>\n",
       "      <td>861</td>\n",
       "      <td>1722</td>\n",
       "      <td>861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1793</td>\n",
       "      <td>1594</td>\n",
       "      <td>1989</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2314</td>\n",
       "      <td>1237</td>\n",
       "      <td>2314</td>\n",
       "      <td>2314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>303</td>\n",
       "      <td>359</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>960</td>\n",
       "      <td>2519</td>\n",
       "      <td>2519</td>\n",
       "      <td>960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1394</td>\n",
       "      <td>186</td>\n",
       "      <td>1237</td>\n",
       "      <td>1394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1892</td>\n",
       "      <td>137</td>\n",
       "      <td>359</td>\n",
       "      <td>2519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1552</td>\n",
       "      <td>1752</td>\n",
       "      <td>960</td>\n",
       "      <td>2436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1296</td>\n",
       "      <td>121</td>\n",
       "      <td>1394</td>\n",
       "      <td>1892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>294</td>\n",
       "      <td>1594</td>\n",
       "      <td>186</td>\n",
       "      <td>1552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>121</td>\n",
       "      <td>2498</td>\n",
       "      <td>137</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1629</td>\n",
       "      <td>1027</td>\n",
       "      <td>2436</td>\n",
       "      <td>1296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>814</td>\n",
       "      <td>814</td>\n",
       "      <td>814</td>\n",
       "      <td>814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1388</td>\n",
       "      <td>1693</td>\n",
       "      <td>1388</td>\n",
       "      <td>1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>180</td>\n",
       "      <td>1023</td>\n",
       "      <td>180</td>\n",
       "      <td>1388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1992</td>\n",
       "      <td>2123</td>\n",
       "      <td>1992</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1133</td>\n",
       "      <td>180</td>\n",
       "      <td>1693</td>\n",
       "      <td>2492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1578</td>\n",
       "      <td>1388</td>\n",
       "      <td>1023</td>\n",
       "      <td>2566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1235</td>\n",
       "      <td>1378</td>\n",
       "      <td>2123</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>497</td>\n",
       "      <td>304</td>\n",
       "      <td>1133</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1779</td>\n",
       "      <td>1790</td>\n",
       "      <td>1578</td>\n",
       "      <td>1023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>981</td>\n",
       "      <td>2566</td>\n",
       "      <td>1378</td>\n",
       "      <td>2484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lexical semantic hybrid creative\n",
       "0     1429     1398   1429     1429\n",
       "1      299     2188   1398      299\n",
       "2      140     1429    299     1689\n",
       "3     1689     2328    140     2328\n",
       "4     2328     2555   1689     1735\n",
       "5     1398      333   2328     1887\n",
       "6     1735      177   2188      140\n",
       "7     1887      410   1735     1834\n",
       "8      410     2410   2555     2477\n",
       "9     2021     1125    410     2205\n",
       "10    1594     1237   1237     1594\n",
       "11    1237     1722   1621     1237\n",
       "12    1621     1989   1594     1621\n",
       "13     316     1621    316      316\n",
       "14    2208      510   2208     2208\n",
       "15      15     1752     15       15\n",
       "16    1067     1132    510      510\n",
       "17     510     2073   1067     1067\n",
       "18     122      861   1722      861\n",
       "19    1793     1594   1989      122\n",
       "20    2314     1237   2314     2314\n",
       "21     303      359    303      303\n",
       "22     960     2519   2519      960\n",
       "23    1394      186   1237     1394\n",
       "24    1892      137    359     2519\n",
       "25    1552     1752    960     2436\n",
       "26    1296      121   1394     1892\n",
       "27     294     1594    186     1552\n",
       "28     121     2498    137      121\n",
       "29    1629     1027   2436     1296\n",
       "30     814      814    814      814\n",
       "31    1388     1693   1388     1992\n",
       "32     180     1023    180     1388\n",
       "33    1992     2123   1992      180\n",
       "34    1133      180   1693     2492\n",
       "35    1578     1388   1023     2566\n",
       "36    1235     1378   2123      304\n",
       "37     497      304   1133     1790\n",
       "38    1779     1790   1578     1023\n",
       "39     981     2566   1378     2484"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def generate_exploded_df(search_results_df):\n",
    "#     exploded_search_results_df = pd.concat([search_results_df[col].explode().reset_index() for col in search_results_df.columns], axis=1)\n",
    "#     exploded_search_results_df = exploded_search_results_df.apply(lambda l: [doc_id for doc_id, _ in l])\n",
    "#     return exploded_search_results_df\n",
    "\n",
    "def generate_exploded_df(search_results_df):\n",
    "\n",
    "    df_doc_ids = search_results_df.applymap(\n",
    "        lambda lst: [doc_id for doc_id, _ in lst] if isinstance(lst, list) else []\n",
    "    )\n",
    "\n",
    "    exploded_df = pd.DataFrame({\n",
    "        col: df_doc_ids[col].explode().reset_index(drop=True)\n",
    "        for col in df_doc_ids.columns\n",
    "    })\n",
    "\n",
    "    return exploded_df\n",
    "\n",
    "def generate_found_docs_text_df(exploded_search_results_df, all_docs_df):\n",
    "    # Recupera os ids únicos dos documentos\n",
    "    documents_ids = set(exploded_search_results_df.to_numpy().flatten().tolist())\n",
    "\n",
    "    # Salva os textos e os ids dos documetnos que foram encontrados ems usas buscas\n",
    "    documents_df = all_docs_df[all_docs_df[\"doc_id\"].isin(documents_ids)][[\"Texto processado\", \"doc_id\"]]\n",
    "    return documents_df\n",
    "\n",
    "exploded_df = generate_exploded_df(search_results_df)\n",
    "found_docs_text_df = generate_found_docs_text_df(exploded_df, all_docs_df=df)\n",
    "\n",
    "def save_results_to_file(exploded_df: pd.DataFrame,\n",
    "                         found_docs_text_df: pd.DataFrame,\n",
    "                         exploded_df_save_filepath: str = \"data/search_results.csv\",\n",
    "                         found_docs_text_save_filepath: str = \"data/documents.csv\"):\n",
    "    exploded_df.to_csv(exploded_df_save_filepath)\n",
    "    found_docs_text_df.to_csv(found_docs_text_save_filepath)\n",
    "    print(\"Resultados das buscas salvos em 'data/search_results.csv'.\")\n",
    "    print(\"Documentos de interesse salvos em 'data/documents.csv'.\")\n",
    "    \n",
    "save_results_to_file(exploded_df, found_docs_text_df)\n",
    "exploded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>É falso que houve megaprotesto na Alemanha con...</td>\n",
       "      <td>circular rede social foto mostrar dezena carro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Estudo não aponta que carga viral de vacinados...</td>\n",
       "      <td>circular rede social estudo publicar revista c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>É antiga foto de manifestante em cima de carro...</td>\n",
       "      <td>circular rede social foto homem mascarar , seg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Diretor da Anvisa não pediu demissão e critico...</td>\n",
       "      <td>circular rede social diretor anvisa , agencia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Jogadoras da seleção de futebol não posaram in...</td>\n",
       "      <td>circular rede imagem dois jogadora selecao bra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>2565</td>\n",
       "      <td>Es falso que farmacias de Italia estén distrib...</td>\n",
       "      <td>circula en las rede sociales las farmacias en ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2566</th>\n",
       "      <td>2566</td>\n",
       "      <td>Foto viral que mostra Faixa de Gaza após bomba...</td>\n",
       "      <td>circular rede social foto mostrar cidade ceu c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2567</th>\n",
       "      <td>2567</td>\n",
       "      <td>É falso que STF afastou Bolsonaro do controle ...</td>\n",
       "      <td>circular rede social supremo tribunal federal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2568</th>\n",
       "      <td>2568</td>\n",
       "      <td>É falso que Magazine Luiza 'financia' fome dos...</td>\n",
       "      <td>em o comissao parlamentar inquerito ( cpi ) on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2569</th>\n",
       "      <td>2569</td>\n",
       "      <td>Como a nova faixa de isenção impactará no Impo...</td>\n",
       "      <td>em    fevereiro , receita federal anunciar mud...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2570 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      doc_id                                              title  \\\n",
       "0          0  É falso que houve megaprotesto na Alemanha con...   \n",
       "1          1  Estudo não aponta que carga viral de vacinados...   \n",
       "2          2  É antiga foto de manifestante em cima de carro...   \n",
       "3          3  Diretor da Anvisa não pediu demissão e critico...   \n",
       "4          4  Jogadoras da seleção de futebol não posaram in...   \n",
       "...      ...                                                ...   \n",
       "2565    2565  Es falso que farmacias de Italia estén distrib...   \n",
       "2566    2566  Foto viral que mostra Faixa de Gaza após bomba...   \n",
       "2567    2567  É falso que STF afastou Bolsonaro do controle ...   \n",
       "2568    2568  É falso que Magazine Luiza 'financia' fome dos...   \n",
       "2569    2569  Como a nova faixa de isenção impactará no Impo...   \n",
       "\n",
       "                                                content  \n",
       "0     circular rede social foto mostrar dezena carro...  \n",
       "1     circular rede social estudo publicar revista c...  \n",
       "2     circular rede social foto homem mascarar , seg...  \n",
       "3     circular rede social diretor anvisa , agencia ...  \n",
       "4     circular rede imagem dois jogadora selecao bra...  \n",
       "...                                                 ...  \n",
       "2565  circula en las rede sociales las farmacias en ...  \n",
       "2566  circular rede social foto mostrar cidade ceu c...  \n",
       "2567  circular rede social supremo tribunal federal ...  \n",
       "2568  em o comissao parlamentar inquerito ( cpi ) on...  \n",
       "2569  em    fevereiro , receita federal anunciar mud...  \n",
       "\n",
       "[2570 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_id_map(all_docs_df, output_csv_path):\n",
    "    \"\"\"\n",
    "    Exports a stable mapping of ALL documents used in Elasticsearch.\n",
    "\n",
    "    The doc_id values will exactly match those returned by search results.\n",
    "    \"\"\"\n",
    "\n",
    "    required_cols = {\"doc_id\", \"Título\", \"Texto processado\"}\n",
    "    missing = required_cols - set(all_docs_df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    id_map_df = (\n",
    "        all_docs_df[[\"doc_id\", \"Título\", \"Texto processado\"]]\n",
    "        .rename(columns={\n",
    "            \"Título\": \"title\",\n",
    "            \"Texto processado\": \"content\"\n",
    "        })\n",
    "        .sort_values(\"doc_id\")   # optional, but good for reproducibility\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    id_map_df.to_csv(output_csv_path, sep=\";\", index=False)\n",
    "\n",
    "    return id_map_df\n",
    "\n",
    "generate_id_map(df, \"data/id_map.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elastic-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
